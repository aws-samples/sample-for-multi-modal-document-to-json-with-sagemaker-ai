{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1d4af7-18dc-4a11-88b0-ef7165b58b10",
   "metadata": {},
   "source": [
    "# Fine-tuning Multi-Modal Language Models with Amazon SageMaker AI\n",
    "\n",
    "This notebook demonstrates how to fine-tune large language models using Amazon SageMaker AI \n",
    "and the Modelscope Swift framework. The process includes:\n",
    "\n",
    "1. Setting up model and training configurations\n",
    "2. Configuring SageMaker resources\n",
    "3. Fine-tuning the model\n",
    "4. Evaluating the fine-tuning training process\n",
    "5. Downloading and analyzing the fine-tuned model\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- Model Configuration: Select and configure the model to be fine-tuned\n",
    "- SageMaker Setup: Configure AWS resources and training environment\n",
    "- Training Process: Fine-tune the model using the SWIFT framework\n",
    "- Evaluation: Analyze training metrics and model performance\n",
    "- Model Export: Save and prepare the model for deployment\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- AWS SageMaker access with appropriate permissions\n",
    "- Training data in the correct format\n",
    "- Sufficient GPU resources for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0b83a-2c31-4fe8-8e14-11f7a686b1d7",
   "metadata": {},
   "source": [
    "The training will happen inside a container image. We decided to use a SageMaker Distribution image because it already contains many useful dependencies, for example a pytorch installation. \n",
    "\n",
    "\n",
    "You can learn more about SageMaker Distribution images in the [SageMaker Distribution documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html#notebooks-available-images-arn).\n",
    "\n",
    "Here are a few example distributions from the link above:\n",
    "* us-east-1: 885854791233.dkr.ecr.us-east-1.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\n",
    "* us-west-2: 542918446943.dkr.ecr.us-west-2.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\n",
    "\n",
    "Later for the training you will get to know the @remote decorator. The @remote decorator requires your local python version to be the same as the one in the training image. Let's try to find a suitable SageMaker Distribution image for your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109420b-33fd-4703-aa2e-8ec50ea412d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils.training_image import (\n",
    "    get_sagemaker_distribution, \n",
    "    SageMakerDistribution, \n",
    "    get_python_version, \n",
    "    get_aws_account_id_for_region,\n",
    "    is_docker_installed,\n",
    "    is_docker_compose_installed,\n",
    "    check_and_enable_docker_access_sagemaker_studio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173962f-c79b-4133-ac64-7152cd29f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_version = sys.version_info\n",
    "print(f\"Your Python version: {str(get_python_version(*py_version))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7792a-fa47-49e3-81f4-c0f0b67cdcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_distro_version = get_sagemaker_distribution(py_version)\n",
    "print(f\"Using SageMaker distribution v{sm_distro_version.image_version} as training image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f1099-07a9-4df3-b27f-d53ce58e58b0",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489abc5c-5e5f-45fb-b1df-6878dbc5224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_sdk_version = sm_distro_version.sagemaker_python_sdk # local SageMaker version must be same as in training job with remote decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056c026-c36f-4901-90ff-442af4cb25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U --quiet requests beautifulsoup4 dataclasses\n",
    "%pip install --quiet sagemaker=={sagemaker_sdk_version} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325cc6fd-6751-4e87-8eeb-79ae3cc9ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, Union, List\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e99938-8e7e-49d9-bfa8-6c263290b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.remote_function import remote, CheckpointLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0cec6-2a65-4418-947f-63c0868d6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import ModelConfig\n",
    "from utils.finetuning import (\n",
    "    check_checkpoints_directory,\n",
    "    find_latest_version_dir,\n",
    "    find_latest_checkpoint,\n",
    "    find_latest_checkpoint_path,\n",
    "    get_latest_sagemaker_training_job,\n",
    "    get_s3_suffix,\n",
    "    find_best_model_checkpoint\n",
    ")\n",
    "from utils.model_manager import list_available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c55ec7-c7b5-4dd2-9cc6-7b9f677adf5d",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0387f6-bc80-4469-b110-c65d0bf38327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Qwen2.5-VL-7B for its strong performance on vision-language tasks\n",
    "# Can be a vision model that MS Swift supports: \n",
    "# https://github.com/modelscope/ms-swift/blob/main/docs/source_en/Instruction/Supported-models-and-datasets.md\n",
    "model_config = ModelConfig(\n",
    "    model_type=\"qwen2_5_vl\",\n",
    "    model_id=\"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "    # Other models are\n",
    "    # model_type = \"deepseek_janus_pro\",\n",
    "    # model_id = \"deepseek-ai/Janus-Pro-7B\"\n",
    "    \n",
    "    # model_type = \"qwen2_vl\",\n",
    "    # model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    \n",
    "    # model_type = \"qwen2_5_vl\",\n",
    "    # model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    \n",
    "    # model_type = \"llama3_2_vision\",\n",
    "    # model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Configured model id.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb1fdf-8aa2-4271-b8d1-ae7d3bdb4663",
   "metadata": {},
   "source": [
    "### SageMaker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec01fa-88d5-4d36-ae60-71ad532fc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session and configure AWS resources for training\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    region = session.boto_region_name\n",
    "    \n",
    "    # Configure S3 paths for data and artifacts\n",
    "    # CHANGE if your dataset is in a different S3 bucket\n",
    "    default_bucket_name = session.default_bucket()\n",
    "    dataset_s3_prefix = \"fatura2-train-data\"\n",
    "    s3_root_uri = f\"s3://{default_bucket_name}\"\n",
    "    dataset_s3_uri = f\"{s3_root_uri}/{dataset_s3_prefix}\"\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error setting up SageMaker session: {str(e)}\")\n",
    "print(\"âœ… Initialized SageMaker session...\")\n",
    "print(f\"ðŸ’¾ Using dataset: {dataset_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb810a4-6d30-4221-8159-ddea55e2a49c",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e2e87-9d90-421d-bdeb-4b7a36bb1a46",
   "metadata": {},
   "source": [
    "Next you will configure the training job.\n",
    "First up is which container image to use during training and which dependencies to install into he container image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19099de-3f5e-4d28-a511-94c9160bf7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_distr_account_id = get_aws_account_id_for_region(region)\n",
    "if not sagemaker_distr_account_id:\n",
    "    raise ValueError(\n",
    "        f\"Please make sure to manually set the `sagemaker_distr_account_id` account id for your specific AWS region ({region}) from the AWS documentation: https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html#notebooks-available-images-arn\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5343aa6-311e-4a51-8c35-73c1a231bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define the sagemaker distribution to use\n",
    "sagemaker_dist_uri = f\"{sagemaker_distr_account_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-distribution-prod:{sm_distro_version.image_version}-gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e607a-28e1-4aad-8fa3-836efadc0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = f\"\"\"git+https://github.com/huggingface/accelerate.git@v1.7.0\n",
    "ms-swift@git+https://github.com/modelscope/ms-swift.git@v3.5.3\n",
    "git+https://github.com/huggingface/transformers@v4.52.4\n",
    "av\n",
    "qwen_vl_utils==0.0.11\n",
    "decord\n",
    "optimum\n",
    "huggingface_hub[hf_transfer]\n",
    "tensorboardX\n",
    "tensorboard\n",
    "sagemaker=={sagemaker_sdk_version}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7f197-4c39-4966-9c4b-70b0065055af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dependencies >requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667254a1-3265-4b59-ba68-c018bff70650",
   "metadata": {},
   "source": [
    "The instance type required will depend on which model you are using, the hyperparameters, and the dataset size. If you encounter an out of memory error then you should use a larger instance type or change the training configuration. Here are the instance types we have been using:\n",
    "| Instance Type  | Model | Optimized | Note |\n",
    "|----------------|-------|-----------|------|\n",
    "| ml.g6.8xlarge | Qwen/Qwen2.5-VL-3B-Instruct | Optimized | This is with 300 training samples. If you have a larger dataset you might need a bigger instance, for example ml.g6e.8xlarge or ml.g6.12xlarge |\n",
    "| ml.g6e.48xlarge | meta-llama/Llama-3.2-11B-Vision-Instruct | Not optimized | You can probably use a smaller instance type |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be59ec2-c713-4a2e-b4d0-b2fde4fdbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_local_mode = True # Set to true to run on local instance\n",
    "instance_type = \"local_gpu\" if use_local_mode else \"ml.g6.8xlarge\" # \"ml.g6.12xlarge\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f4c36-677f-41a3-9b1e-7448edc0c789",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #006CE0; \n",
    "    padding: 10px; \n",
    "    border-radius: 5px; \n",
    "    max-width: 100%;\n",
    "    background: #f0fbff;\">\n",
    "    <b>Note:</b> If you run into out of memory errors during training use a larger instance type. For example if you are training a larger model or with more data. If you are using local mode then you will need a GPU on your local machine, for example running inside a SageMaker Studio JuypterLab on a ml.g6.8xlarge instance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84434e34-6a64-4618-93b7-1acb9592c2e1",
   "metadata": {},
   "source": [
    "We can use Spot Instances for training. A Spot Instance depends on availability and might get interrupted if others need the capacity. The advantage of Spot Instances is up to 90% price reduction compared to the on demand price. SageMaker takes care of restarting the training job once capacity is back available. Training with checkpointing goes well together with Spot Instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a85ab-6484-4b6c-9731-8ea0bd5d262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92b5e9-1f0a-488a-9a62-fba0838a37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training job parameters and checkpoint management\n",
    "training_job_name_prefix = model_config.training_job_prefix(dataset_s3_prefix)\n",
    "print(f\"Training job name prefix: {training_job_name_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1d6fa-5271-4f5c-9161-4a4242cf2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_s3_uri = os.path.join(s3_root_uri, training_job_name_prefix, \"checkpoints\")\n",
    "checkpoint_loc = CheckpointLocation(s3_uri=checkpoint_s3_uri)\n",
    "print(f\"Checkpoint S3 location: {checkpoint_loc._s3_uri}\")\n",
    "latest_checkpoint = find_latest_checkpoint_path(checkpoint_loc._s3_uri)\n",
    "print(f\"Training will use checkpoint: {latest_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661b3cf-d73b-489d-b0dc-4e2ce5c46f65",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #006CE0; \n",
    "    padding: 10px; \n",
    "    border-radius: 5px; \n",
    "    max-width: 100%;\n",
    "    background: #f0fbff;\">\n",
    "    <b>Note:</b> Please check the checkpoint configuration above. Checkpointing is useful if you want to continue training from a previous training runs checkpoint or if you are using Spot Instances to recover from interruptions. If you do not want to use checkpointing then you should set `checkpoint_loc` to None or delete the content at the S3 location with the command in the cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c1089-cc0c-4576-948b-d335e6b980ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_loc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7b8a0-ae39-4630-b1c6-954308b9b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 rm --recursive --quiet {checkpoint_loc._s3_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488ad0a-e91d-46c9-a950-d0f213f4c233",
   "metadata": {},
   "source": [
    "By default, the [Amazon SageMaker Python SDK reads configuration](https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk) values from an admin defined or user specific configuration file. This configuration allows all kind of customizations do be made. Setting the `SAGEMAKER_USER_CONFIG_OVERRIDE` environment variable below overwrites these defaults. The main settings you will configure below are\n",
    "\n",
    "* The container image URI that should run the remote function code.\n",
    "* Python dependencies to install for the remote training.\n",
    "* Which files from the local working directory not to upload to the remote code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ee743-7661-4f6e-aa45-1412917ec674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override user config to ensure consistent environment setup\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c086cca-bd24-4ebf-b4cb-d65a11ca5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the environment variables for the training\n",
    "env_variables ={\n",
    "    \"SIZE_FACTOR\": json.dumps(8), # can be increase but requires more GPU memory\n",
    "    \"MAX_PIXELS\": json.dumps(602112), # can be increase but requires more GPU memory\n",
    "    \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3\", # depends on the instance type, ml.g6.8xlarge has 4 GPUs\n",
    "    \"NPROC_PER_NODE\": \"4\", # depends on the instance type, ml.g6.8xlarge has 4 GPUs\n",
    "    \"USE_HF_TRANSFER\": json.dumps(1),\n",
    "    # \"HF_TOKEN\": \"xxxxxxxxxxx\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5fc0f-f6b2-4199-837a-10b9b3c3e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_yaml = f\"\"\"\n",
    "SchemaVersion: '1.0'\n",
    "SageMaker:\n",
    "  PythonSDK:\n",
    "    Modules:\n",
    "      RemoteFunction:\n",
    "        # role arn is not required if in SageMaker Notebook instance or SageMaker Studio\n",
    "        # Uncomment the following line and replace with the right execution role if in a local IDE\n",
    "        # RoleArn: <replace the role arn here>\n",
    "        S3RootUri: {s3_root_uri}\n",
    "        ImageUri: {sagemaker_dist_uri}        \n",
    "        InstanceType: {instance_type} # default instance type to use\n",
    "        Dependencies: ./requirements.txt\n",
    "        IncludeLocalWorkDir: true\n",
    "        PreExecutionCommands:\n",
    "        - \"pip install packaging\"\n",
    "        - \"sudo mkdir -p /opt/ml/cache\"\n",
    "        - \"sudo chmod -R 777 /opt/ml/cache\"\n",
    "        - \"sudo mkdir -p /opt/ml/checkpoints\"\n",
    "        - \"sudo chmod -R 777 /opt/ml/checkpoints\"\n",
    "        - \"sudo mkdir -p /opt/ml/model\"\n",
    "        - \"sudo chmod -R 777 /opt/ml/model\"\n",
    "        - \"sudo chown sagemaker-user:sagemaker-user /opt/ml/model\"\n",
    "        - \"echo 'Granted checkpoints directory permissions'\"\n",
    "        CustomFileFilter:\n",
    "          IgnoreNamePatterns:\n",
    "          - \"*.ipynb\"\n",
    "          - \"__pycache__\"\n",
    "          - \"data\"\n",
    "          - \"images\"\n",
    "          - \"bin\"\n",
    "          - \"models\"\n",
    "          - \"results\"\n",
    "          - \".git\"\n",
    "        EnvironmentVariables: {json.dumps(env_variables)}\n",
    "        Tags:\n",
    "          - Key: 'purpose'\n",
    "            Value: 'fine-tuning'\n",
    "          - Key: 'model_id'\n",
    "            Value: {model_config.model_id}\n",
    "          - Key: 'dataset'\n",
    "            Value: {dataset_s3_uri}\n",
    "\"\"\"\n",
    "\n",
    "print(config_yaml, file=open(\"config.yaml\", \"w\"))\n",
    "print(config_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb20f2-4a1b-461e-988b-9d19c929105a",
   "metadata": {},
   "source": [
    "For the training you will need to set hyperparameters. We have already set sensible defaults for the parameters. You can overwrite any of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b3b31c-ccf0-40ba-9068-a45e5892e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_kwargs = {\n",
    "    \"training_data_s3\":dataset_s3_uri,\n",
    "    \"checkpoint_loc\":checkpoint_loc,\n",
    "    \"model_type\":model_config.model_type,\n",
    "    \"model_id\":model_config.model_id,\n",
    "    \"train_data_path\":\"conversations_train_swift_format.json\", \n",
    "    \"validation_data_path\":\"conversations_dev_swift_format.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8b129-b5b8-4e60-8a94-90111526e159",
   "metadata": {},
   "source": [
    "### Defining fine-tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4ebf9-06e5-4afd-b985-bac9956a0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @remote(instance_type=\"ml.g6e.48xlarge\", volume_size=200, use_spot_instances=True,job_name_prefix=training_job_name_prefix, max_wait_time_in_seconds=172800,max_runtime_in_seconds=172800)\n",
    "def fine_tune_documents(\n",
    "    model_type: str, model_id: str, checkpoint_loc: Optional[CheckpointLocation], \n",
    "    training_data_s3: str, train_data_path=\"train.jsonl\", validation_data_path=\"validation.jsonl\"\n",
    ") -> str:\n",
    "    \"\"\"Fine-tune model with checkpoint recovery support for cost-efficient spot training.\n",
    "    \n",
    "    The fine-tuning is a 3 step process:\n",
    "    1. Download the training data.\n",
    "    2. Configure the fine-tuning\n",
    "    3. Run the fine-tuning\n",
    "            \n",
    "    \"\"\"\n",
    "    import os\n",
    "    from swift.llm import sft_main, TrainArguments\n",
    "    import shutil\n",
    "    from utils.finetuning import find_latest_checkpoint_path, setup_directories\n",
    "\n",
    "\n",
    "    output_dir = os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\")\n",
    "    checkpoint_dir = checkpoint_loc._local_path if checkpoint_loc else output_dir # directory for checkpoint artifacts (for spot training or to continue previous training)\n",
    "    dataset_dir = \".\"\n",
    "    \n",
    "\n",
    "   \n",
    "    setup_directories(output_dir, checkpoint_dir, dataset_dir)\n",
    "\n",
    "    \n",
    "    # 1. Copy training data into the training container\n",
    "    subprocess.run(\n",
    "        [\"aws\", \"s3\", \"cp\", training_data_s3, dataset_dir, \"--recursive\", \"--quiet\"],\n",
    "        check=True,\n",
    "        shell=False\n",
    "    )\n",
    "    \n",
    "    train_data_local_path = os.path.join(dataset_dir, train_data_path)\n",
    "    validation_data_local_path = os.path.join(dataset_dir, validation_data_path)\n",
    "\n",
    "\n",
    "    from transformers import modeling_utils\n",
    "    if not hasattr(modeling_utils, \"ALL_PARALLEL_STYLES\") or modeling_utils.ALL_PARALLEL_STYLES is None:\n",
    "        modeling_utils.ALL_PARALLEL_STYLES = [\"tp\", \"none\", \"colwise\", \"rowwise\"]\n",
    "    \n",
    "    # 2. Define training parameters\n",
    "    # swift sft ...\n",
    "    argv = [\n",
    "        \"--model_type\", model_type,\n",
    "        \"--model\", model_id,\n",
    "        \"--model_revision\", \"main\", # We recommend that you pin to a specific commit from HuggingFace Hub\n",
    "        \"--train_type\", \"lora\",\n",
    "        \"--use_dora\", \"true\",\n",
    "        \"--output_dir\", checkpoint_dir,\n",
    "        \"--max_length\", \"4096\",\n",
    "        \"--dataset\", train_data_local_path,\n",
    "        \"--val_dataset\", validation_data_local_path,\n",
    "        \"--save_steps\", \"50\",\n",
    "        \"--logging_steps\",\"5\",\n",
    "        \"--num_train_epochs\", \"4\",\n",
    "        \"--lora_dtype\", \"bfloat16\",\n",
    "        \"--per_device_train_batch_size\", \"4\",\n",
    "        \"--per_device_eval_batch_size\", \"1\",\n",
    "        \"--learning_rate\", \"1e-4\", # \"4.0e-5\", #  \"2e-4\"\n",
    "        \"--target_modules\", \"all-linear\",\n",
    "        \"--use_hf\", \"true\",\n",
    "        \"--warmup_ratio\",\"0.05\",\n",
    "        \"--save_total_limit\",\"3\",\n",
    "        \"--gradient_accumulation_steps\",\"1\",\n",
    "        \"--freeze_vit\", \"true\", # default: true\n",
    "        \"--freeze_llm\", \"false\", # default: false\n",
    "        \"--freeze_aligner\", \"true\" # default: true\n",
    "    ]\n",
    "\n",
    "    # Find latest checkpoint for training recovery\n",
    "    full_checkpoint_path = find_latest_checkpoint_path(checkpoint_dir)\n",
    "    if full_checkpoint_path:\n",
    "        argv.append(\"--resume_from_checkpoint\")\n",
    "        argv.append(full_checkpoint_path)\n",
    "\n",
    "    # 3. Execute training\n",
    "    result = sft_main(argv)\n",
    "    best_checkpoint = result[\"best_model_checkpoint\"]\n",
    "\n",
    "    if checkpoint_loc:\n",
    "        # Copy training artifacts to SageMaker output directory\n",
    "        shutil.copytree(checkpoint_dir, output_dir, dirs_exist_ok=True)\n",
    "        \n",
    "    return best_checkpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd5418-9d48-4ef6-a057-9d8d88be1761",
   "metadata": {},
   "source": [
    "## Define SageMaker Pipeline for Local Mode training\n",
    "\n",
    "This approach allows us to execute the Sagemaker training job in LocalMode, without having to wait for any remote instances or resources.\n",
    "Make sure you have a Jupyterlab space with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00b8ef-6005-4786-bcd7-eb3c38c532db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# import mlflow\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "\n",
    "def run_pipeline(local_mode=True):\n",
    "    train_result = step(fine_tune_documents, name=\"finetune\")(**fine_tuning_kwargs)\n",
    "    \n",
    "    steps = [train_result]\n",
    "    \n",
    "    role = get_execution_role()\n",
    "    local_pipeline_session = LocalPipelineSession()\n",
    "    more_params = {}\n",
    "    if local_mode:\n",
    "        more_params[\"sagemaker_session\"] = local_pipeline_session \n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        name=training_job_name_prefix,\n",
    "        parameters=[],\n",
    "        steps=steps,\n",
    "        pipeline_definition_config=PipelineDefinitionConfig(use_custom_job_prefix=True),\n",
    "        **more_params\n",
    "    )\n",
    "\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98a280-723e-4cbc-913a-8b3ed6189ab0",
   "metadata": {},
   "source": [
    "## Run Fine-Tuning Remote or Local Mode\n",
    "\n",
    "Now we can run the fine-tuning with the `RemoteExecutor` as a SageMaker training job or we can run the fine-tuning locally with the pipeline in local mode. \n",
    "\n",
    "Note: We could also run the Pipeline on Amazon SageMaker. We decided not to because pipelines usually have multiple steps but our pipeline only has one training step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40f9f3-ee0d-4ba0-8472-f6bfde86503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_local_mode and (not is_docker_installed() or not is_docker_compose_installed()): \n",
    "    # we need docker and docker-compose for LocalMode execution\n",
    "    !bash docker-artifacts/01_docker_install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39563e42-1a02-4c14-af64-68101a055633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if in SageMaker Studio check if docker access is enabled if it is not enable it\n",
    "check_and_enable_docker_access_sagemaker_studio(use_local_mode, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f7e72-8f44-4389-b02d-645df25bb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_local_mode:\n",
    "    print(\"\\nStarting fine-tuning locally...\")\n",
    "    run_pipeline(local_mode=True)\n",
    "else:\n",
    "    # run remotely\n",
    "    from sagemaker.remote_function import RemoteExecutor\n",
    "    \n",
    "    with RemoteExecutor(instance_type=instance_type, volume_size=200, use_spot_instances=use_spot,job_name_prefix=training_job_name_prefix, max_wait_time_in_seconds=172800,max_runtime_in_seconds=172800) as job:\n",
    "        print(\"\\nStarting fine-tuning process remotely...\")\n",
    "        print(\n",
    "            f\"View your job here: https://{region}.console.aws.amazon.com/sagemaker/home?region={region}#/jobs/\"\n",
    "        )\n",
    "        future = job.submit(fine_tune_documents, **fine_tuning_kwargs)\n",
    "        result = future.result()\n",
    "        print(f\"Fine-tuning remote completed: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a939b-2fca-463d-9512-29a920594f88",
   "metadata": {},
   "source": [
    "## Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b64db-e055-446b-9fc9-b6acb2e13258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list_available_models(default_bucket_name, training_job_name_prefix)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b42874-0bb7-433b-8e7d-2a2d9acbdb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_model_to_pick = 0 # use first model from list by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbd582-eb13-4eaa-91e3-762db102ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the S3 URI from which we will download the model\n",
    "model_key=df['Key'].iloc[which_model_to_pick]\n",
    "model_output_url = f\"s3://{default_bucket_name}/{model_key}\"\n",
    "print(f\"Selected model for download: {model_key}\")\n",
    "print(f\"S3 Model URI: {model_output_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6210b-2b3c-4dfb-8a8d-bd02103ca95a",
   "metadata": {},
   "source": [
    "We copy the model from S3 to our local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f96d38-93d6-420b-b714-d3a7c3013ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import get_s3_suffix\n",
    "\n",
    "model_suffix_s3 = get_s3_suffix(model_output_url)\n",
    "model_weights_dir = \"./models\"\n",
    "model_destination = f\"{model_weights_dir}/{model_suffix_s3}\"\n",
    "model_dest_dir = str(Path(model_destination).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0425e9-a06d-4455-8f54-36cfd1918ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {model_output_url} {model_destination}\n",
    "!tar --warning=no-unknown-keyword  -xzvf {model_destination} --directory {model_dest_dir} > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea35e4d-f35a-4ee0-aac4-6d93e0540fd2",
   "metadata": {},
   "source": [
    "Lets have a look what is inside of model.tar.gz:\n",
    "\n",
    "* The checkpoint directory contains the actual adapter\n",
    "* adapter_model.safetensors - contains the actual weights of the adapter\n",
    "  \n",
    "For inference you could either use the adapter together with the original model, or we merge the adapter with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeea338-0f33-46b5-8240-50effffeddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {model_dest_dir} && du -ah --max-depth=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1410ff-9edf-4280-bb17-be0aad5ce593",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = model_dest_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c6132-5e5e-4391-ba0a-973776c41fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import find_latest_version_directory, find_best_model_checkpoint\n",
    "\n",
    "latest_version = find_latest_version_directory(model_dir)\n",
    "latest_model_dir = os.path.join(model_dir, latest_version)\n",
    "logging_file = os.path.join(os.getcwd(), model_dir, latest_version, \"logging.jsonl\")\n",
    "best_model_checkpoint = find_best_model_checkpoint(logging_file)\n",
    "if best_model_checkpoint:\n",
    "    best_model_checkpoint = best_model_checkpoint.replace(\"/opt/ml/model/\", \"\")\n",
    "    print(f\"best model checkpoint: {best_model_checkpoint}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Best model checkpoint not found. Please search the logs manually to find the path that stores the best model checkpoint.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794b931-160d-4679-ad52-6cc9ff3b9196",
   "metadata": {},
   "source": [
    "## View Evaluation Metrics from fine-tuning run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e644f42f-3f6b-489e-a8b2-3df542e23c1e",
   "metadata": {},
   "source": [
    "Next you can look at the train & evaluation accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef038ad7-c721-42a5-9f5b-8d5a445562cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(latest_model_dir, \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaff447-340d-4c75-9502-caeaa070642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_image(images_dir, image):\n",
    "    image = Image(os.path.join(images_dir, image))\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727ef04-cc79-4249-bbfb-2fb4a1684a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(images_dir, \"train_token_acc.png\")\n",
    "display_image(images_dir, \"train_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37956c-abcb-4f7f-9065-66d5081966d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(images_dir, \"eval_token_acc.png\")\n",
    "display_image(images_dir, \"eval_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374a951-c09a-4cbf-9655-d274cf5462d8",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc99f54-9368-4e69-9d22-ce8a49eb99b4",
   "metadata": {},
   "source": [
    "1. Run inference on unseen data to evaluate the models real-world performance: [04_run_batch_inference.ipynb](04_run_batch_inference.ipynb) and then [05_evaluate_model.ipynb](05_evaluate_model.ipynb).\n",
    "2. Deploy the model: [06_deploy_model_endpoint.ipynb](06_deploy_model_endpoint.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2540a-ad51-4fbb-9446-96d3e353bdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
