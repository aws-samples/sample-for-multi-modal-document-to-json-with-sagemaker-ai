{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1d4af7-18dc-4a11-88b0-ef7165b58b10",
   "metadata": {},
   "source": [
    "# Fine-tuning Multi-Modal Language Models with Amazon SageMaker AI\n",
    "\n",
    "This notebook demonstrates how to fine-tune large language models using Amazon SageMaker AI \n",
    "and the Modelscope Swift framework. The process includes:\n",
    "\n",
    "1. Setting up model and training configurations\n",
    "2. Configuring SageMaker resources\n",
    "3. Fine-tuning the model\n",
    "4. Evaluating the fine-tuning training process\n",
    "5. Downloading and analyzing the fine-tuned model\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- Model Configuration: Select and configure the model to be fine-tuned\n",
    "- SageMaker Setup: Configure AWS resources and training environment\n",
    "- Training Process: Fine-tune the model using the SWIFT framework\n",
    "- Evaluation: Analyze training metrics and model performance\n",
    "- Model Export: Save and prepare the model for deployment\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- AWS SageMaker access with appropriate permissions\n",
    "- Training data in the correct format\n",
    "- Sufficient GPU resources for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056c026-c36f-4901-90ff-442af4cb25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker==2.227.0 -U --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325cc6fd-6751-4e87-8eeb-79ae3cc9ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e99938-8e7e-49d9-bfa8-6c263290b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.remote_function import remote, CheckpointLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0cec6-2a65-4418-947f-63c0868d6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import ModelConfig\n",
    "from utils.finetuning import (\n",
    "    check_checkpoints_directory,\n",
    "    find_latest_version_dir,\n",
    "    find_latest_checkpoint,\n",
    "    get_latest_sagemaker_training_job,\n",
    "    get_s3_suffix,\n",
    "    find_best_model_checkpoint\n",
    ")\n",
    "from utils.model_manager import list_available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c55ec7-c7b5-4dd2-9cc6-7b9f677adf5d",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0387f6-bc80-4469-b110-c65d0bf38327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Qwen2.5-VL-7B for its strong performance on vision-language tasks\n",
    "# Can be a vision model that MS Swift supports: \n",
    "# https://github.com/modelscope/ms-swift/blob/main/docs/source_en/Instruction/Supported-models-and-datasets.md\n",
    "model_config = ModelConfig(\n",
    "    model_type=\"qwen2_5_vl\",\n",
    "    model_id=\"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "    # Other models are\n",
    "    # model_type = \"deepseek_janus_pro\",\n",
    "    # model_id = \"deepseek-ai/Janus-Pro-7B\"\n",
    "    \n",
    "    # model_type = \"qwen2_vl\",\n",
    "    # model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    \n",
    "    # model_type = \"qwen2_5_vl\",\n",
    "    # model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    \n",
    "    # model_type = \"llama3_2_vision\",\n",
    "    # model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Configured model id.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb1fdf-8aa2-4271-b8d1-ae7d3bdb4663",
   "metadata": {},
   "source": [
    "### SageMaker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec01fa-88d5-4d36-ae60-71ad532fc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session and configure AWS resources for training\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    region = session.boto_region_name\n",
    "    \n",
    "    # Configure S3 paths for data and artifacts\n",
    "    # CHANGE if your dataset is in a different S3 bucket\n",
    "    default_bucket_name = session.default_bucket()\n",
    "    dataset_s3_prefix = \"fatura2-train-data\"\n",
    "    s3_root_uri = f\"s3://{default_bucket_name}\"\n",
    "    dataset_s3_uri = f\"{s3_root_uri}/{dataset_s3_prefix}\"\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error setting up SageMaker session: {str(e)}\")\n",
    "print(\"âœ… Initialized SageMaker session...\")\n",
    "print(f\"ðŸ’¾ Using dataset: {dataset_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb810a4-6d30-4221-8159-ddea55e2a49c",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44b85d-c1a9-45b5-b3d7-40ab3ee3d352",
   "metadata": {},
   "source": [
    "Lets define the SageMaker distribution image to be used for us-east-1. The URI for other distributions and regions can be found in the [SageMaker Distribution documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html#notebooks-available-images-arn).\n",
    "\n",
    "Here are a few example distributions from the link above:\n",
    "\n",
    "* us-east-1: 885854791233.dkr.ecr.us-east-1.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\n",
    "* us-west-2: 542918446943.dkr.ecr.us-west-2.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44833254-0b11-4ee5-91e6-a166e4ab8d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define the sagemaker distribution to use\n",
    "if region == \"us-east-1\":\n",
    "    sagemaker_dist_uri = \"885854791233.dkr.ecr.us-east-1.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\"\n",
    "elif region == \"us-west-2\":\n",
    "    sagemaker_dist_uri = \"542918446943.dkr.ecr.us-west-2.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\"\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Please make sure to manually set the `sagemaker_dist_uri` uri for your specific AWS region from the AWS documentation: https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html#notebooks-available-images-arn\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7f197-4c39-4966-9c4b-70b0065055af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./requirements.txt\n",
    "\n",
    "git+https://github.com/huggingface/accelerate.git@v1.5.2\n",
    "ms-swift@git+https://github.com/modelscope/ms-swift.git@v3.2.0\n",
    "git+https://github.com/huggingface/transformers@v4.49.0\n",
    "pyav\n",
    "qwen_vl_utils==0.0.10\n",
    "decord\n",
    "optimum\n",
    "huggingface_hub[hf_transfer]\n",
    "tensorboardX\n",
    "tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be59ec2-c713-4a2e-b4d0-b2fde4fdbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_local_mode = False # Set to true to run on local instance\n",
    "instance_type = \"local_gpu\" if use_local_mode else \"ml.g6e.48xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84434e34-6a64-4618-93b7-1acb9592c2e1",
   "metadata": {},
   "source": [
    "We can use Spot Instances for training. A Spot Instance depends on availability and might get interrupted if others need the capacity. The advantage of Spot Instances is up to 90% price reduction compared to the on demand price. SageMaker takes care of restarting the training job once capacity is back available. Training with checkpointing goes well together with Spot Instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a85ab-6484-4b6c-9731-8ea0bd5d262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92b5e9-1f0a-488a-9a62-fba0838a37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training job parameters and checkpoint management\n",
    "training_job_name_prefix = model_config.training_job_prefix(dataset_s3_prefix)\n",
    "print(f\"Training job name prefix: {training_job_name_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1d6fa-5271-4f5c-9161-4a4242cf2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_s3_uri = os.path.join(s3_root_uri, training_job_name_prefix, \"checkpoints\")\n",
    "checkpoint_loc = CheckpointLocation(s3_uri=checkpoint_s3_uri)\n",
    "print(f\"Checkpoint S3 location: {checkpoint_loc._s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488ad0a-e91d-46c9-a950-d0f213f4c233",
   "metadata": {},
   "source": [
    "By default, the [Amazon SageMaker Python SDK reads configuration](https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk) values from an admin defined or user specific configuration file. This configuration allows all kind of customizations do be made. Setting the `SAGEMAKER_USER_CONFIG_OVERRIDE` environment variable below overwrites these defaults. The main settings you will configure below are\n",
    "\n",
    "* The container image URI that should run the remote function code.\n",
    "* Python dependencies to install for the remote training.\n",
    "* Which files from the local working directory not to upload to the remote code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ee743-7661-4f6e-aa45-1412917ec674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override user config to ensure consistent environment setup\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c086cca-bd24-4ebf-b4cb-d65a11ca5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the environment variables for the training\n",
    "env_variables ={\n",
    "    \"SIZE_FACTOR\": json.dumps(8), # can be increase but requires more GPU memory\n",
    "    \"MAX_PIXELS\": json.dumps(602112), # can be increase but requires more GPU memory\n",
    "    \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\", # depends on the instance type, ml.g5.48xlarge has 8 GPUs\n",
    "    \"NPROC_PER_NODE\": \"8\", # depends on the instance type, ml.g5.48xlarge has 8 GPUs\n",
    "    \"USE_HF_TRANSFER\": json.dumps(1),\n",
    "    # \"HF_TOKEN\": \"xxxxxxxxxxx\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5fc0f-f6b2-4199-837a-10b9b3c3e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_yaml = f\"\"\"\n",
    "SchemaVersion: '1.0'\n",
    "SageMaker:\n",
    "  PythonSDK:\n",
    "    Modules:\n",
    "      RemoteFunction:\n",
    "        # role arn is not required if in SageMaker Notebook instance or SageMaker Studio\n",
    "        # Uncomment the following line and replace with the right execution role if in a local IDE\n",
    "        # RoleArn: <replace the role arn here>\n",
    "        S3RootUri: {s3_root_uri}\n",
    "        ImageUri: {sagemaker_dist_uri}        \n",
    "        InstanceType: {instance_type} # default instance type to use\n",
    "        Dependencies: ./requirements.txt\n",
    "        IncludeLocalWorkDir: true\n",
    "        PreExecutionCommands:\n",
    "        - \"pip install packaging\"\n",
    "        - \"sudo mkdir -p /opt/ml/cache\"\n",
    "        - \"sudo chmod -R 777 /opt/ml/cache\"\n",
    "        - \"sudo mkdir -p /opt/ml/checkpoints\"\n",
    "        - \"sudo chmod -R 777 /opt/ml/checkpoints\"\n",
    "        - \"sudo mkdir -p /opt/ml/model\"\n",
    "        - \"sudo chmod -R 777 /opt/ml/model\"\n",
    "        - \"sudo chown sagemaker-user:sagemaker-user /opt/ml/model\"\n",
    "        - \"echo 'Granted checkpoints directory permissions'\"\n",
    "        CustomFileFilter:\n",
    "          IgnoreNamePatterns:\n",
    "          - \"*.ipynb\"\n",
    "          - \"__pycache__\"\n",
    "          - \"data\"\n",
    "          - \"bin\"\n",
    "          - \"models\"\n",
    "          - \"results\"\n",
    "          - \".git\"\n",
    "        EnvironmentVariables: {json.dumps(env_variables)}\n",
    "        Tags:\n",
    "          - Key: 'purpose'\n",
    "            Value: 'fine-tuning'\n",
    "          - Key: 'model_id'\n",
    "            Value: {model_config.model_id}\n",
    "          - Key: 'dataset'\n",
    "            Value: {dataset_s3_uri}\n",
    "\"\"\"\n",
    "\n",
    "print(config_yaml, file=open(\"config.yaml\", \"w\"))\n",
    "print(config_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb20f2-4a1b-461e-988b-9d19c929105a",
   "metadata": {},
   "source": [
    "For the training you will need to set hyperparameters. We have already set sensible defaults for the parameters. You can overwrite any of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b3b31c-ccf0-40ba-9068-a45e5892e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_kwargs = {\n",
    "    \"training_data_s3\":dataset_s3_uri,\n",
    "    \"checkpoint_loc\":checkpoint_loc,\n",
    "    \"model_type\":model_config.model_type,\n",
    "    \"model_id\":model_config.model_id,\n",
    "    \"train_data_path\":\"conversations_train_swift_format.json\", \n",
    "    \"validation_data_path\":\"conversations_dev_swift_format.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8b129-b5b8-4e60-8a94-90111526e159",
   "metadata": {},
   "source": [
    "### Defining fine-tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4ebf9-06e5-4afd-b985-bac9956a0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @remote(instance_type=\"ml.g6e.48xlarge\", volume_size=200, use_spot_instances=True,job_name_prefix=training_job_name_prefix, max_wait_time_in_seconds=172800,max_runtime_in_seconds=172800)\n",
    "def fine_tune_documents(\n",
    "    model_type: str, model_id: str, checkpoint_loc: CheckpointLocation, \n",
    "    training_data_s3: str, train_data_path=\"train.jsonl\", validation_data_path=\"validation.jsonl\"\n",
    ") -> str:\n",
    "    \"\"\"Fine-tune model with checkpoint recovery support for cost-efficient spot training.\n",
    "    \n",
    "    The fine-tuning is a 3 step process:\n",
    "    1. Download the training data.\n",
    "    2. Configure the fine-tuning\n",
    "    3. Run the fine-tuning\n",
    "            \n",
    "    \"\"\"\n",
    "    import os\n",
    "    from swift.llm import sft_main, TrainArguments\n",
    "    import shutil\n",
    "    from utils.finetuning import find_latest_checkpoint_path, setup_directories\n",
    "\n",
    "\n",
    "    checkpoint_dir = checkpoint_loc._local_path # directory for checkpoint artifacts (for spot training)\n",
    "    dataset_dir = \".\"\n",
    "    output_dir = os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\")\n",
    "\n",
    "   \n",
    "    setup_directories(output_dir, checkpoint_dir, dataset_dir)\n",
    "\n",
    "    \n",
    "    # 1. Copy training data into the training container\n",
    "    subprocess.run(\n",
    "        [\"aws\", \"s3\", \"cp\", training_data_s3, dataset_dir, \"--recursive\", \"--quiet\"],\n",
    "        check=True,\n",
    "        shell=False\n",
    "    )\n",
    "    \n",
    "    train_data_local_path = os.path.join(dataset_dir, train_data_path)\n",
    "    validation_data_local_path = os.path.join(dataset_dir, validation_data_path)\n",
    "\n",
    "    # define training parameters\n",
    "    #swift sft ...\n",
    "    argv = [\n",
    "        \"--model_type\", model_type,\n",
    "        \"--model\", model_id,\n",
    "        \"--train_type\", \"lora\",\n",
    "        # \"--use_dora\", \"true\",\n",
    "        \"--output_dir\", checkpoint_dir,\n",
    "        \"--max_length\", \"4096\",\n",
    "        \"--dataset\", train_data_local_path,\n",
    "        \"--val_dataset\", validation_data_local_path,\n",
    "        \"--save_steps\", \"50\",\n",
    "        \"--logging_steps\",\"5\",\n",
    "        \"--num_train_epochs\", \"4\",\n",
    "        \"--lora_dtype\", \"bfloat16\",\n",
    "        \"--per_device_train_batch_size\", \"4\",\n",
    "        \"--per_device_eval_batch_size\", \"1\",\n",
    "        \"--learning_rate\", \"1e-4\", # \"4.0e-5\", #  \"2e-4\"\n",
    "        \"--target_modules\", \"all-linear\",\n",
    "        \"--use_hf\", \"true\",\n",
    "        \"--warmup_ratio\",\"0.05\",\n",
    "        \"--save_total_limit\",\"3\",\n",
    "        \"--gradient_accumulation_steps\",\"2\"\n",
    "    ]\n",
    "\n",
    "    # Find latest checkpoint for training recovery\n",
    "    full_checkpoint_path = find_latest_checkpoint_path(checkpoint_dir)\n",
    "    if full_checkpoint_path:\n",
    "        argv.append(\"--resume_from_checkpoint\")\n",
    "        argv.append(full_checkpoint_path)\n",
    "\n",
    "    # 3. Execute training\n",
    "    result = sft_main(argv)\n",
    "    best_checkpoint = result[\"best_model_checkpoint\"]\n",
    "    \n",
    "    # Copy training artifacts to SageMaker output directory\n",
    "    shutil.copytree(checkpoint_dir, output_dir, dirs_exist_ok=True)\n",
    "    return best_checkpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd5418-9d48-4ef6-a057-9d8d88be1761",
   "metadata": {},
   "source": [
    "## Define SageMaker Pipeline for Local Mode training\n",
    "\n",
    "This approach allows us to execute the Sagemaker training job in LocalMode, without having to wait for any remote instances or resources.\n",
    "Make sure you have a Jupyterlab space with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00b8ef-6005-4786-bcd7-eb3c38c532db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# import mlflow\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "\n",
    "def run_pipeline(local_mode=True):\n",
    "    train_result = step(fine_tune_documents, name=\"finetune\")(**fine_tuning_kwargs)\n",
    "    \n",
    "    steps = [train_result]\n",
    "    \n",
    "    role = get_execution_role()\n",
    "    local_pipeline_session = LocalPipelineSession()\n",
    "    more_params = {}\n",
    "    if local_mode:\n",
    "        more_params[\"sagemaker_session\"] = local_pipeline_session \n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        name=training_job_name_prefix,\n",
    "        parameters=[],\n",
    "        steps=steps,\n",
    "        pipeline_definition_config=PipelineDefinitionConfig(use_custom_job_prefix=True),\n",
    "        **more_params\n",
    "    )\n",
    "\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98a280-723e-4cbc-913a-8b3ed6189ab0",
   "metadata": {},
   "source": [
    "## Run Fine-Tuning Remote or Local Mode\n",
    "\n",
    "Now we can run the fine-tuning with the `RemoteExecutor` as a SageMaker training job or we can run the fine-tuning locally with the pipeline in local mode. \n",
    "\n",
    "Note: We could also run the Pipeline on Amazon SageMaker. We decided not to because pipelines usually have multiple steps but our pipeline only has one training step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40f9f3-ee0d-4ba0-8472-f6bfde86503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_local_mode: # we need docker and docker-compose for LocalMode execution\n",
    "    !bash docker-artifacts/01_docker_install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f7e72-8f44-4389-b02d-645df25bb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_local_mode:\n",
    "    print(\"\\nStarting fine-tuning locally...\")\n",
    "    run_pipeline(local_mode=True)\n",
    "else:\n",
    "    # run remotely\n",
    "    from sagemaker.remote_function import RemoteExecutor\n",
    "    \n",
    "    with RemoteExecutor(instance_type=instance_type, volume_size=200, use_spot_instances=use_spot,job_name_prefix=training_job_name_prefix, max_wait_time_in_seconds=172800,max_runtime_in_seconds=172800) as job:\n",
    "        print(\"\\nStarting fine-tuning process...\")\n",
    "        print(\n",
    "            f\"View your job here: https://{region}.console.aws.amazon.com/sagemaker/home?region={region}#/jobs/\"\n",
    "        )\n",
    "        future = job.submit(fine_tune_documents, **fine_tuning_kwargs)\n",
    "        result = future.result()\n",
    "        print(f\"Fine-tuning remote completed: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a939b-2fca-463d-9512-29a920594f88",
   "metadata": {},
   "source": [
    "## Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b64db-e055-446b-9fc9-b6acb2e13258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list_available_models(default_bucket_name, training_job_name_prefix)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b42874-0bb7-433b-8e7d-2a2d9acbdb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_model_to_pick = 0 # use first model from list by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbd582-eb13-4eaa-91e3-762db102ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the S3 URI from which we will download the model\n",
    "model_key=df['Key'].iloc[which_model_to_pick]\n",
    "model_output_url = f\"s3://{default_bucket_name}/{model_key}\"\n",
    "print(f\"Selected model for download: {model_key}\")\n",
    "print(f\"S3 Model URI: {model_output_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6210b-2b3c-4dfb-8a8d-bd02103ca95a",
   "metadata": {},
   "source": [
    "We copy the model from S3 to our local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f96d38-93d6-420b-b714-d3a7c3013ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import get_s3_suffix\n",
    "\n",
    "model_suffix_s3 = get_s3_suffix(model_output_url)\n",
    "model_weights_dir = \"./models\"\n",
    "model_destination = f\"{model_weights_dir}/{model_suffix_s3}\"\n",
    "model_dest_dir = str(Path(model_destination).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0425e9-a06d-4455-8f54-36cfd1918ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {model_output_url} {model_destination}\n",
    "!tar --warning=no-unknown-keyword  -xzvf {model_destination} --directory {model_dest_dir} > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea35e4d-f35a-4ee0-aac4-6d93e0540fd2",
   "metadata": {},
   "source": [
    "Lets have a look what is inside of model.tar.gz:\n",
    "\n",
    "* The checkpoint directory contains the actual adapter\n",
    "* adapter_model.safetensors - contains the actual weights of the adapter\n",
    "  \n",
    "For inference you could either use the adapter together with the original model, or we merge the adapter with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeea338-0f33-46b5-8240-50effffeddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {model_dest_dir} && du -ah --max-depth=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1410ff-9edf-4280-bb17-be0aad5ce593",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = model_dest_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c6132-5e5e-4391-ba0a-973776c41fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import find_latest_version_directory, find_best_model_checkpoint\n",
    "\n",
    "latest_version = find_latest_version_directory(model_dir)\n",
    "latest_model_dir = os.path.join(model_dir, latest_version)\n",
    "logging_file = os.path.join(os.getcwd(), model_dir, latest_version, \"logging.jsonl\")\n",
    "best_model_checkpoint = find_best_model_checkpoint(logging_file)\n",
    "if best_model_checkpoint:\n",
    "    best_model_checkpoint = best_model_checkpoint.replace(\"/opt/ml/model/\", \"\")\n",
    "    print(f\"best model checkpoint: {best_model_checkpoint}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Best model checkpoint not found. Please search the logs manually to find the path that stores the best model checkpoint.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794b931-160d-4679-ad52-6cc9ff3b9196",
   "metadata": {},
   "source": [
    "## View Evaluation Metrics during fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e644f42f-3f6b-489e-a8b2-3df542e23c1e",
   "metadata": {},
   "source": [
    "Next you can look at the train & evaluation accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef038ad7-c721-42a5-9f5b-8d5a445562cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(latest_model_dir, \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaff447-340d-4c75-9502-caeaa070642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_image(images_dir, image):\n",
    "    image = Image(os.path.join(images_dir, image))\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727ef04-cc79-4249-bbfb-2fb4a1684a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(images_dir, \"train_token_acc.png\")\n",
    "display_image(images_dir, \"train_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37956c-abcb-4f7f-9065-66d5081966d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(images_dir, \"eval_token_acc.png\")\n",
    "display_image(images_dir, \"eval_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374a951-c09a-4cbf-9655-d274cf5462d8",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc99f54-9368-4e69-9d22-ce8a49eb99b4",
   "metadata": {},
   "source": [
    "1. Run inference on unseen data to evaluate the models real-world performance: [04_run_batch_inference.ipynb](04_run_batch_inference.ipynb) and then [05_evaluate_model.ipynb](05_evaluate_model.ipynb).\n",
    "2. Deploy the model: [06_deploy_model_endpoint.ipynb](06_deploy_model_endpoint.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2540a-ad51-4fbb-9446-96d3e353bdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
