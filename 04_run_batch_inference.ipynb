{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f6ad8f-1048-4084-8d1c-d8b2da850b37",
   "metadata": {},
   "source": [
    "# Batch Inference for Fine-tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc657673-24c1-44e3-88fc-df1f4ecbcfba",
   "metadata": {},
   "source": [
    "This notebook implements batch inference for fine-tuned models using Amaizn SageMaker AI. We use batch inference because:\n",
    "1. We want to run inference on the test dataset that the model has not seen during training for evaluation.\n",
    "2. Real-time inference would be too costly and slow for our dataset size\n",
    "3. We use Amazon SageMaker AI @remote decorator to have the option to run inference in the local environment or with GPU instances remotely without code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a13148-983e-4e5f-89c2-95948c31096a",
   "metadata": {},
   "source": [
    "The notebook handles:\n",
    "1. Setting up the environment and configurations\n",
    "2. Loading and preparing the model\n",
    "3. Running batch inference\n",
    "4. Downloading inference results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b482c-b271-4659-be0e-c27a260b19c0",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- AWS credentials configured\n",
    "- Access to Amazon SageMaker AI training jobs\n",
    "- Sufficient quota for a GPU instance for SageMaker training job or spot training job\n",
    "- Fine-tuned model artifacts in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789abd84-6610-4487-8dc2-922ce08419d6",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce73740-3892-4782-b037-956866a6bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker==2.227.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a05264-0e19-4d84-8673-4192bd7533e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018eef40-5823-4f6d-b65b-554819517184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Union, Dict, Optional\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.remote_function import remote\n",
    "\n",
    "\n",
    "import csv\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from utils.config import ModelConfig\n",
    "from utils.helpers import get_s3_suffix, shorten_for_sagemaker_training_job\n",
    "from utils.model_manager import list_available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9b96d-fb7d-4dd0-9aa1-e1389f376131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session and configure AWS resources for training\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    region = session.boto_region_name\n",
    "    \n",
    "    # Configure S3 paths for data and artifacts\n",
    "    # CHANGE if your dataset is in a different S3 bucket\n",
    "    default_bucket_name = session.default_bucket()\n",
    "    dataset_s3_prefix = \"fatura2-train-data\"\n",
    "    s3_root_uri = f\"s3://{default_bucket_name}\"\n",
    "    dataset_s3_uri = f\"{s3_root_uri}/{dataset_s3_prefix}\"\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error setting up SageMaker session: {str(e)}\")\n",
    "print(\"âœ… Initialized SageMaker session...\")\n",
    "print(f\"ðŸ’¾ Using dataset for inference: {dataset_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88613b56-90a1-4b3c-a42f-92c6f1a51327",
   "metadata": {},
   "source": [
    "## Retrieve model artifact location from fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a3f31-95f8-4608-b0b0-d34fa12aa9a4",
   "metadata": {},
   "source": [
    "We retrieve the latest fine-tuning training job from Amazon SageMaker AI to retrieve the models artifacts location.\n",
    "\n",
    "We need to track training jobs because:\n",
    "1. We want to use the latest fine-tuned model\n",
    "2. Job names help in resource organization\n",
    "3. We need to locate model artifacts in S3\n",
    "4. Version tracking is crucial for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7bebe-69e1-48a3-8d83-86c583fa3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models that you fine-tuned.\n",
    "base_model_config = ModelConfig(\n",
    "    # Replace with model type and model id of the base model.\n",
    "    model_type=\"qwen2_5_vl\",\n",
    "    model_id=\"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "    # model_type = \"llama3_2_vision\",\n",
    "    # model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Configured model id.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67d3cb-73f2-4051-9f9b-1cf3511aeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name_prefix = base_model_config.training_job_prefix(dataset_s3_prefix)\n",
    "print(f\"Fine-tuning name prefix: {training_job_name_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00598990-0184-43d7-b93f-47c869a99c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"<div style=\"background-color: #f3fd91; border: 2px solid #a2bb11; padding: 10px; color: black; font-family: Arial, Helvetica, sans-serif;\">\n",
    "    Skip the next 4 cells below if you want to run inference using <b>{base_model_config.model_id}</b> from HuggingFace Hub. Run cells below if you want to use a model that you have fine-tuned.\n",
    "</div>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b16bc-3941-4234-a118-2107025d7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list_available_models(default_bucket_name, training_job_name_prefix)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289743de-b05f-44a8-bca5-8dbe7b93411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_model_to_pick = 0 # use first model from list by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33403ff0-ad8e-487f-a6f7-5938ad51e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the S3 URI from which we will download the model\n",
    "model_key=df['Key'].iloc[which_model_to_pick]\n",
    "model_output_url = f\"s3://{default_bucket_name}/{model_key}\"\n",
    "print(f\"Selected model for deployment: {model_key}\")\n",
    "print(f\"S3 Model URI: {model_output_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5f1ee-fdfa-4977-8ed3-f7a62cbb3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_suffix_s3 = get_s3_suffix(model_output_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb9abf-af74-4e07-9f83-1258073ce0bf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f3fd91; border: 2px solid #a2bb11; padding: 10px; color: black; font-family: Arial, Helvetica, sans-serif;\">\n",
    "    Continue below for inference with base model or fine-tuned model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a4bdb-ee89-46fa-ae1f-0676498e35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_config = ModelConfig(\n",
    "        # Replace with model type and model id of the base model.\n",
    "        model_type=base_model_config.model_type,\n",
    "        model_id=model_output_url\n",
    "    )\n",
    "    \n",
    "    prefix = model_suffix_s3.split(\"/\")[0]\n",
    "    print(\"âœ… Configured fine-tuned model id.\")\n",
    "    \n",
    "except NameError:\n",
    "    # not using fine-tuned model\n",
    "    model_config = base_model_config\n",
    "    prefix = model_config.model_id.replace(\"/\",\"-\").replace(\".\",\"-\")\n",
    "    print(\"âœ… Using base model for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb837e37-5c00-45e7-8e35-080137f13850",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model for inference: {model_config.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381b4eb-f79a-45c3-9812-aa11f1f93a13",
   "metadata": {},
   "source": [
    "## Configure Job for Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a09c32-6fe0-417b-a668-d33369233600",
   "metadata": {},
   "source": [
    "Lets define the SageMaker distribution image to be used for us-east-1. The URI for other distributions and regions can be found in the [SageMaker Distribution documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html#notebooks-available-images-arn).\n",
    "\n",
    "Here are a few example distributions from the link above:\n",
    "\n",
    "* us-east-1: 885854791233.dkr.ecr.us-east-1.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\n",
    "* us-west-2: 542918446943.dkr.ecr.us-west-2.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b96110-c983-4a12-a3a7-ef289db9428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define the sagemaker distribution to use\n",
    "if region == \"us-east-1\":\n",
    "    sagemaker_dist_uri = \"885854791233.dkr.ecr.us-east-1.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\"\n",
    "elif region == \"us-west-2\":\n",
    "    sagemaker_dist_uri = \"542918446943.dkr.ecr.us-west-2.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\"\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Please make sure to manually set the `sagemaker_dist_uri` uri for your specific AWS region using the provided link from the cell above.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202dfb1b-322c-470d-9191-29490a7ffee6",
   "metadata": {},
   "source": [
    "Define the dependencies that are required during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a431390-b5bf-4f29-8ba4-b98b76ca8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./requirements.txt\n",
    "git+https://github.com/huggingface/accelerate.git@v1.5.2\n",
    "ms-swift@git+https://github.com/modelscope/ms-swift.git@v3.2.0\n",
    "git+https://github.com/huggingface/transformers.git@014047e1c8784c00e2a04cb04ffcecdd5cb23c16\n",
    "pyav\n",
    "vllm==0.7.2\n",
    "decord\n",
    "optimum\n",
    "qwen-vl-utils==0.0.10\n",
    "huggingface_hub\n",
    "hf_transfer\n",
    "xgrammar # grammar constrained decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4266ad0-e08f-4879-8c15-fe8c0d06b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_root_uri = \"s3://{}\".format(default_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2feb3-ffa3-4802-8d42-765aeefcf853",
   "metadata": {},
   "source": [
    "### Environment Variables Configuration\n",
    "\n",
    "We set specific environment variables because:\n",
    "1. Memory usage needs to be optimized for GPUs\n",
    "2. Image processing has size constraints\n",
    "3. We want faster downloads from Hugging Face\n",
    "4. Resource limits need to be carefully managed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01f39a-5c69-4019-87c0-addfe9bc558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the environment variables for the training\n",
    "env_variables ={\n",
    "    \"SIZE_FACTOR\": json.dumps(8), # can be increase but requires more GPU memory\n",
    "    \"MAX_PIXELS\": json.dumps(602112), # can be increase but requires more GPU memory\n",
    "    \"USE_HF_TRANSFER\": json.dumps(1),\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER\": json.dumps(1),\n",
    "    # \"HF_TOKEN\": \"xxxxxxxx\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8094be-e688-4282-9734-4eb7427a3a57",
   "metadata": {},
   "source": [
    "### SageMaker Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9f8c0-c8aa-447a-9e20-ecd550f68310",
   "metadata": {},
   "source": [
    "By default, the [Amazon SageMaker Python SDK reads configuration](https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk) values from an admin defined or user specific configuration file. This configuration allows all kind of customizations do be made. Setting the `SAGEMAKER_USER_CONFIG_OVERRIDE` environment variable below overwrites these defaults. The main settings you will configure below are\n",
    "\n",
    "* The container image URI that should run the remote function code.\n",
    "* Python dependencies to install for the remote training.\n",
    "* Which files from the local working directory not to upload to the remote code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b5708-0c71-46b8-896c-c8e0239e248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb2018-3a25-4d9e-af7b-52ffced1c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_yaml = f\"\"\"\n",
    "SchemaVersion: '1.0'\n",
    "SageMaker:\n",
    "  PythonSDK:\n",
    "    Modules:\n",
    "      RemoteFunction:\n",
    "        # role arn is not required if in SageMaker Notebook instance or SageMaker Studio\n",
    "        # Uncomment the following line and replace with the right execution role if in a local IDE\n",
    "        # RoleArn: <replace the role arn here>\n",
    "        S3RootUri: {s3_root_uri}\n",
    "        ImageUri: {sagemaker_dist_uri}        \n",
    "        InstanceType: ml.g6e.2xlarge\n",
    "        Dependencies: ./requirements.txt\n",
    "        IncludeLocalWorkDir: true\n",
    "        PreExecutionCommands:\n",
    "        - \"pip install packaging\"\n",
    "        CustomFileFilter:\n",
    "          IgnoreNamePatterns:\n",
    "          - \"*.ipynb\"\n",
    "          - \"__pycache__\"\n",
    "          - \"data\"\n",
    "          - \"venv\"\n",
    "          - \"bin\"\n",
    "          - \"models\"\n",
    "          - \"results\"\n",
    "        EnvironmentVariables: {json.dumps(env_variables)}\n",
    "        Tags:\n",
    "          - Key: 'purpose'\n",
    "            Value: 'inference'\n",
    "          - Key: 'model_id'\n",
    "            Value: {model_config.model_id}\n",
    "          - Key: 'dataset'\n",
    "            Value: {dataset_s3_uri}\n",
    "\"\"\"\n",
    "\n",
    "print(config_yaml, file=open(\"config.yaml\", \"w\"))\n",
    "print(config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabc5f1-4548-472c-a1f0-23f625e44350",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = shorten_for_sagemaker_training_job(f\"infer-json-{prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ab2c2-ea11-4ede-a21a-f317e1c95722",
   "metadata": {},
   "source": [
    "### Constrained Decoding\n",
    "\n",
    "Constrained decoding controls a language model's next-token prediction process by limiting which tokens it can generate to only those that satisfy specific rules or formats. During the normal generation process, a language model assigns probabilities to all possible next tokens. With constrained decoding the set of next tokens is limited to only tokens that satisfy the required structure. For example with JSON constrained decoding the model can only select tokens that create a valid JSON syntax. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2aae81-b505-4724-8279-1ecd3c028831",
   "metadata": {},
   "source": [
    "Below you can configure constrained decoding for the batch inference:\n",
    "1. Set it to `None` to run batch inference without any constrained decoding.\n",
    "2. If you have a JSON schema file in your dataset you can set `guided_decoding` to the path of that JSON schema file inside your dataset, for example `guided_decoding = \"groundtruth_schema.json\"`. You can reference the [02_create_custom_dataset_swift.ipynb](02_create_custom_dataset_swift.ipynb) notebook on how to create a JSON schema file. \n",
    "3. You can also set `guided_decoding` to a dict sturctured output parameter from the [vLLM documentation](https://docs.vllm.ai/en/latest/features/structured_outputs.html), for example `guided_decoding = {\"guided_choice\": [\"positive\", \"negative\"]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ac4b8-96a9-4c0a-b5dc-d336933fb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_decoding = None # 1. default no constrained decoding\n",
    "\n",
    "# guided_decoding = \"groundtruth_schema.json\" # 2. use a JSON schema inside dataset\n",
    "\n",
    "# 3. Below is an example on how to configure structure output in accordance to the vLLM documentation\n",
    "# from pydantic import BaseModel\n",
    "\n",
    "# class Invoice(BaseModel):\n",
    "#     purpose: str\n",
    "#     amount: int\n",
    "\n",
    "# json_schema = Invoice.model_json_schema()\n",
    "# guided_decoding = {\"guided_json\": json_schema}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2d6b5-6d8a-40c9-a3e3-bb998e0ce05a",
   "metadata": {},
   "source": [
    "## Batch Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b57369-4ac0-47ac-9df1-75a23d5e9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@remote(\n",
    "    instance_type=\"ml.g6e.xlarge\",  # Powerful GPU for fast inference\n",
    "    instance_count=1,  # Single instance for cost efficiency\n",
    "    volume_size=200, # Large volume for model and data storage\n",
    "    job_name_prefix=job_name_prefix,\n",
    "    use_spot_instances=True, # Cost efficient inference. Inference can be restarted if no spot capacity. \n",
    "    max_wait_time_in_seconds=172800, # 48 hours max wait\n",
    "    max_runtime_in_seconds=172800, # 48 hours max runtime\n",
    ")\n",
    "def batch_inference(\n",
    "    model_id: str,\n",
    "    model_type: str,\n",
    "    dataset_s3: str,\n",
    "    test_data_path: str = \"test.jsonl\",\n",
    "    guided_decoding: Optional[Union[Dict, str]] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run batch inference using SageMaker.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Model identifier or S3 URI\n",
    "        model_type: Type of the model\n",
    "        dataset_s3: S3 URI for the dataset\n",
    "        test_data_path: Path to the test data file\n",
    "        guided_decoding: vllm guided_decoding config or path to json schema. Default: None - no constrained decoding used\n",
    "        \n",
    "    Returns:\n",
    "        Status message\n",
    "    \"\"\"\n",
    "    from utils.model_manager import ModelManager\n",
    "    from swift.llm import infer_main\n",
    "    from pathlib import Path\n",
    "    import subprocess\n",
    "    import json\n",
    "\n",
    "\n",
    "    output_dir = Path(\"/opt/ml/model\")\n",
    "    \n",
    "    # copy the training data from input source to local directory\n",
    "    dataset_dir = Path(\".\")\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    subprocess.run(\n",
    "        [\"aws\", \"s3\", \"cp\", dataset_s3, dataset_dir, \"--recursive\", \"--quiet\"],\n",
    "        shell = False\n",
    "    )\n",
    "    \n",
    "    test_data_local_path = dataset_dir / test_data_path\n",
    "    result_path = output_dir / \"results.jsonl\"\n",
    "    \n",
    "    model_manager = ModelManager()\n",
    "    guided_decoding = model_manager.construct_guided_decoding_config(dataset_dir, guided_decoding)\n",
    "    \n",
    "    argv = [\n",
    "        \"--result_path\", str(result_path),\n",
    "        \"--max_length\", \"4096\",  # Maximum sequence length for processing\n",
    "        \"--val_dataset\", str(test_data_local_path),\n",
    "        \"--use_hf\", \"true\", \n",
    "        \"--infer_backend\", \"vllm\",  # Use VLLM for faster inference\n",
    "        \"--gpu_memory_utilization\", \"0.98\",  # High GPU utilization for speed\n",
    "        \"--max_num_seqs\", \"8\",  # Batch size for parallel processing\n",
    "        \"--limit_mm_per_prompt\", '{\"image\": 1, \"video\": 0}', # how many images per prompt. Increase if you have multi page pdf\n",
    "        \"--temperature\", \"0\",\n",
    "        # \"--max_model_len\",\"32768\",\n",
    "    ]\n",
    "\n",
    "    model_dir: Path\n",
    "        \n",
    "    # Handle model loading\n",
    "    if model_id.startswith(\"s3://\"):\n",
    "        \n",
    "        model_dir = model_manager.download_and_extract_model(model_id)\n",
    "        ckpt_dir = model_manager.find_best_model_checkpoint(model_dir)\n",
    "       \n",
    "        \n",
    "        model_ckpt_args = [\n",
    "            \"--adapters\", str(ckpt_dir),\n",
    "            \"--merge_lora\", \"true\",\n",
    "            \"--load_data_args\", \"true\"\n",
    "        ]\n",
    "        argv.extend(model_ckpt_args)\n",
    "        \n",
    "    else:\n",
    "        model_dir = model_manager.download_from_hf_hub(model_id)\n",
    "        from_hub_args = [\"--model_type\", model_type, \"--model\", str(model_dir)]\n",
    "        argv.extend(from_hub_args)\n",
    "\n",
    "    model_manager.update_generation_config(model_dir, guided_decoding)\n",
    "\n",
    "    result = infer_main(argv)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb0b491-7f9f-4b83-98d1-daedabe3f311",
   "metadata": {},
   "source": [
    "## Run Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9050ae0-98eb-48c9-8d76-320a5b4ee86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_kwargs = {\n",
    "    \"model_id\":model_config.model_id,\n",
    "    \"model_type\":model_config.model_type,\n",
    "    \"dataset_s3\":dataset_s3_uri,\n",
    "    \"test_data_path\":\"conversations_test_swift_format.json\",\n",
    "    \"guided_decoding\":guided_decoding\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b9e42-49be-4ad9-b33c-b97ba778fc3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"View your job here: https://{region}.console.aws.amazon.com/sagemaker/home?region={region}#/jobs/\"\n",
    ")\n",
    "batch_inference(**inference_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034431a-6e58-456d-8758-f24c2466b401",
   "metadata": {},
   "source": [
    "### Download inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e46ca-bf7f-48cb-948c-a45912875d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import get_latest_sagemaker_training_job\n",
    "\n",
    "job_description = get_latest_sagemaker_training_job(job_name_prefix)\n",
    "\n",
    "# Return the S3 model artifacts path\n",
    "inference_output_url = job_description[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(f\"Inference results can be found at {inference_output_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f426fb-c5b4-4906-802f-46dda8118a74",
   "metadata": {},
   "source": [
    "### Track Inference Results\n",
    "\n",
    "We track inference results in a CSV file for evaluation of different models later.\n",
    "1. We need to maintain a history of all inference runs\n",
    "2. We want to associate results with specific models\n",
    "3. We need to easily locate model outputs later\n",
    "4. CSV format enables easy tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96564905-946d-4b25-a18f-124a116aefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tracking file\n",
    "tracking_file = \"./results_to_compare.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5465ed-ab14-4f57-b901-d2105f64da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPlease enter a human readable name for this inference run:\")\n",
    "human_name = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6100634-226e-4c68-b873-05750293a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5f42c-2ccc-448f-9065-07c3298c5bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Prepare row data\n",
    "    row_data = [human_name, base_model_config.model_id, inference_output_url]\n",
    "    \n",
    "    # Check if file exists\n",
    "    file_exists = os.path.exists(tracking_file)\n",
    "    \n",
    "    # Open file in append mode with proper newline handling\n",
    "    with open(tracking_file, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Write headers if new file\n",
    "        if not file_exists:\n",
    "            writer.writerow(['human_name', 'model', 'inference_results_s3'])\n",
    "            print(f\"âœ… Created new tracking file: {tracking_file}\")\n",
    "            \n",
    "        # Write the new row\n",
    "        writer.writerow(row_data)\n",
    "        print(f\"âœ… Added new inference result to tracking file: {tracking_file}\")\n",
    "    \n",
    "    # Display the full tracking history\n",
    "    print(\"\\nCurrent tracking history:\")\n",
    "    with open(tracking_file, mode='r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        # Get headers for formatting\n",
    "        headers = next(reader)\n",
    "        # Calculate column widths based on content\n",
    "        col_widths = [max(len(str(x)) for x in col) for col in zip([headers], *reader)]\n",
    "        \n",
    "        # Reset file pointer and skip header\n",
    "        f.seek(0)\n",
    "        next(reader)\n",
    "        \n",
    "        # Print headers\n",
    "        header_format = ' | '.join(f'{h:<{w}}' for h, w in zip(headers, col_widths))\n",
    "        print(header_format)\n",
    "        print('-' * len(header_format))\n",
    "        \n",
    "        # Print data rows\n",
    "        for row in reader:\n",
    "            print(' | '.join(f'{x:<{w}}' for x, w in zip(row, col_widths)))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error tracking inference results: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0705b382-cb82-4c02-af0f-4b2710718e2a",
   "metadata": {},
   "source": [
    "## Next step\n",
    "* Continue with the [05_evaluate_model.ipynb](./05_evaluate_model.ipynb) notebook to evaluate the models performance and compare it to other models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
