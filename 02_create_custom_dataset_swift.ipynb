{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Custom Dataset in Swift Format for Training\n",
    "\n",
    "This notebook demonstrates the complete pipeline for processing the Fatura2 invoice dataset into the ModelScope Swift custom format, suitable for training document understanding models. The process includes dataset preparation, image processing, annotation transformation, and cloud storage integration.\n",
    "\n",
    "### Key Features:\n",
    "- Conversion of complex invoice documents to standardized training format\n",
    "- Support for both image and PDF input files\n",
    "- Bounding box normalization and formatting\n",
    "- Integration with Hugging Face Datasets\n",
    "- Full compatibility with Swift training framework\n",
    "\n",
    "### Processing Pipeline:\n",
    "1. Environment setup and dependency installation\n",
    "2. Dataset loading and inspection\n",
    "3. Image extraction and preprocessing\n",
    "4. Annotation transformation to Swift format\n",
    "5. Dataset upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Swift Custom Format\n",
    "\n",
    "The Swift framework requires data in a [specific conversation format with multimodal support](https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html#multimodal). Each training example should contain:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"Task definition\"},\n",
    "    {\"role\": \"user\", \"content\": \"<image><image>... + optional text prompt\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"JSON or text output with extracted data with <bbox> references.\"}\n",
    "  ],\n",
    "  \"images\": [\"path/to/image1.png\", \"path/to/image2.png\"]\n",
    "  \"objects\": {\"ref\": [], \"bbox\": [[90.9, 160.8, 135, 212.8], [360.9, 480.8, 495, 532.8]]}\n",
    "}\n",
    "```\n",
    "\n",
    "### Key Requirements:\n",
    "1. **Multi-image Support**: Multiple images supported by using multiple <image> tags\n",
    "2. **Bounding Box Format**: Coordinates as `[x1,y1,x2,y2]`\n",
    "3. **Image Paths**: Relative paths stored in the `images` array\n",
    "4. **Objects references**: referenced bounding boxes stored in `bbox` array\n",
    "5. **Structured Output**: Nested JSON structure mirroring document hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypdfium2==4.30.1 pandas==2.2.3 huggingface_hub[hf_transfer]==0.27.1 datasets==3.2.0 ipywidgets==8.1.5 tqdm==4.67.1 genson --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Project Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Configure environment for optimal performance\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"  # Enable fast transfers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable tokenizer warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import sagemaker\n",
    "\n",
    "# Authenticate with Hugging Face Hub for private datasets\n",
    "# notebook_login()  # Uncomment for private datasets\n",
    "\n",
    "# Initialize AWS resources\n",
    "session = sagemaker.Session()\n",
    "default_bucket_name = session.default_bucket()\n",
    "dataset_s3_prefix = \"fatura2-train-data\"\n",
    "dataset_s3_uri = f\"s3://{default_bucket_name}/{dataset_s3_prefix}/\"\n",
    "\n",
    "# Create local directory structure\n",
    "data_main_dir = \"./data/\"\n",
    "hf_dataset_name = \"arlind0xbb/Fatura2-invoices-original-strat2\"\n",
    "dataset_dir = os.path.join(data_main_dir, hf_dataset_name.split(\"/\")[-1])\n",
    "os.makedirs(dataset_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading & Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face Hub\n",
    "dataset = load_dataset(hf_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] We reduce the dataset size to 300 samples for faster training and already good enough results\n",
    "# Comment out this cell to train on the full dataset (will require bigger GPU)\n",
    "# Note do not change the test dataset size so that you can compare the evaluation\n",
    "dataset[\"train\"] = dataset[\"train\"].train_test_split(300)[\"test\"]\n",
    "dataset[\"dev\"] = dataset[\"dev\"].train_test_split(75)[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "JSON(json.loads(dataset[\"dev\"].to_pandas()[\"target_data\"].iloc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Processing Pipeline\n",
    "\n",
    "### Key Processing Stages:\n",
    "1. **PDF Rendering**: Convert PDF pages to PNG images at 153% scale for OCR optimization\n",
    "2. **Image Normalization**: Standardize image formats and orientations\n",
    "3. **Bounding Box Transformation**: Convert absolute coordinates to Swift XML format\n",
    "4. **Hierarchy Flattening**: Simplify nested document structures while preserving relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import pypdfium2 as pdfium\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pandas as pd\n",
    "def process_row(row, max_pages, base_dir):\n",
    "    \"\"\"Process document row into Swift-compatible format\"\"\"\n",
    "    filename = Path(row[\"filename\"]).stem\n",
    "    filetype = row[\"filetype\"]\n",
    "    doc_bytes = row[\"doc_bytes\"]\n",
    "\n",
    "    # Configure image output directory\n",
    "    images_dir = os.path.join(base_dir, \"images\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    output_images = []\n",
    "\n",
    "    try:\n",
    "        if filetype.startswith(\"image\"):\n",
    "            # Process single image files\n",
    "            image_path = os.path.join(images_dir, f\"{filename}.png\")\n",
    "            if not os.path.exists(image_path):\n",
    "                Image.open(io.BytesIO(doc_bytes)).save(image_path)\n",
    "            output_images.append(os.path.relpath(image_path, base_dir))\n",
    "        \n",
    "        elif filetype == \"application/pdf\":\n",
    "            # Process PDF documents with multi-page support\n",
    "            pdf = pdfium.PdfDocument(doc_bytes)\n",
    "            for page_number in range(min(len(pdf), max_pages)):\n",
    "                page_path = os.path.join(images_dir, f\"{filename}_page{page_number:03}.png\")\n",
    "                if not os.path.exists(page_path):\n",
    "                    pdf[page_number].render(scale=1.53).to_pil().save(page_path)\n",
    "                output_images.append(os.path.relpath(page_path, base_dir))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    return output_images\n",
    "\n",
    "def transform_annotations(data, defaults=None, remove_bbox=True):\n",
    "    \"\"\"Convert bounding boxes to Swift XML format and simplify structure\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary or list to transform\n",
    "        defaults: List of keys that should exist in first-level dictionaries\n",
    "        remove_bbox: Whether to remove bbox entries from the structure\n",
    "    \"\"\"\n",
    "    bbox_list = []\n",
    "    bbox_pointer = \"<bbox>\"\n",
    "    if isinstance(data, dict):      \n",
    "        # Add missing default keys (only at the first recursion level)\n",
    "        if defaults is not None:\n",
    "            # Use dictionary comprehension for efficiency\n",
    "            missing_keys = {k: None for k in defaults if k not in data}\n",
    "            data.update(missing_keys)\n",
    "        \n",
    "        for key, value in list(data.items()):            \n",
    "            if key == \"bbox\":\n",
    "                if remove_bbox:\n",
    "                    del data[key]  # Skip bbox entries when removing\n",
    "                else:\n",
    "                    # Convert coordinates from {'bbox': [[20.0, 372.8898], [570.0, 282.8898]]} to {'bbox': [20.0, 372.8898, 570.0, 282.8898]}                    \n",
    "                    bbox_value = value[0] + value[1]\n",
    "                    bbox_list.append(bbox_value)\n",
    "                    data[key] = bbox_pointer\n",
    "            else:\n",
    "                value_transformed, bb_list = transform_annotations(value, remove_bbox=remove_bbox)\n",
    "                bbox_list.extend(bb_list)\n",
    "                data[key] = value_transformed\n",
    "        # flatten the object if only one key is left\n",
    "        if len(data) == 1 and \"bbox\" not in data:\n",
    "            data = list(data.values())[0]        \n",
    "            return data, bbox_list\n",
    "        return data, bbox_list\n",
    "        # return {k: v for k, v in data.items() if v is not None}, bbox_list\n",
    "    elif isinstance(data, list):\n",
    "        items = []\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                warnings.warn(\"Ignoring None value in item list: \", data)\n",
    "            else:\n",
    "                item_transformed, bb_list = transform_annotations(item, remove_bbox=remove_bbox)\n",
    "                items.append(item_transformed)\n",
    "                bbox_list.extend(bb_list)\n",
    "        return items, bbox_list\n",
    "    return data, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the transformation on single example entry\n",
    "row = dataset[\"dev\"].to_pandas().iloc[3]\n",
    "target_format, bbox_list = transform_annotations(json.loads(row[\"target_data\"]))\n",
    "JSON(target_format, expanded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets view the bbox_list, contains items if remove_bbox=False\n",
    "JSON(bbox_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Swift Format Conversion\n",
    "\n",
    "### Conversion Logic:\n",
    "1. **Conversation Structure**:\n",
    "   - System: Define document processing task\n",
    "   - User: Provide document images\n",
    "   - Assistant: Return structured JSON\n",
    "   \n",
    "2. **Image Handling**:\n",
    "   - Store relative paths in `images` array\n",
    "   - Reference images in prompt using `<image>{count}` syntax\n",
    "\n",
    "3. **Bounding Box Handling**:\n",
    "   - If activated, store bounding boxes in `\"objects\": {\"bbox\": []}}` array\n",
    "   - Replace the original bbox value with `<bbox>` reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_keys(dataset):\n",
    "    \"\"\"Efficiently collect all keys from all dataset splits\"\"\"\n",
    "    print(\"Collecting all unique keys from all datasets...\")\n",
    "    all_keys = set()\n",
    "    for split_name, dataset_split in dataset.items():\n",
    "        df = dataset_split.to_pandas()\n",
    "        # Extract keys from each target_data JSON and collect unique ones\n",
    "        df[\"target_data\"].apply(json.loads).apply(set).apply(list).explode().drop_duplicates().apply(all_keys.add)\n",
    "        # df[\"target_data\"].apply(extract_keys).apply(all_keys.update)\n",
    "    print(f\"Found {len(all_keys)} unique keys across all datasets\")\n",
    "    return list(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = sorted(collect_all_keys(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def order_json(data):\n",
    "    # Create an OrderedDict with sorted keys\n",
    "    ordered_data = OrderedDict(sorted(data.items()))\n",
    "    return ordered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_swift_example(row):\n",
    "    \"\"\"Construct Swift-compatible training example\"\"\"\n",
    "    conversation = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a document processing expert and assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{'Document pages: <image>'*len(row['images'])} Process all document pages and extract the following information in JSON format: {', '.join(keys)}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": json.dumps(row[\"target_data_clean\"])\n",
    "            }\n",
    "        ],\n",
    "        \"images\": row[\"images\"]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    bbox_list = row[\"bbox_list\"]\n",
    "    if bbox_list and len(bbox_list):\n",
    "        conversation[\"objects\"]: {\"ref\": [], \"bbox\": bbox_list}\n",
    "    \n",
    "    return conversation \n",
    "\n",
    "def convert_dataset(dataset_split, split_name):\n",
    "    \"\"\"Full conversion pipeline for dataset split\"\"\"\n",
    "    df = dataset_split.to_pandas()\n",
    "    \n",
    "    # Process documents and images\n",
    "    df[\"images\"] = [process_row(row, 2, dataset_dir) for _, row in tqdm(df.iterrows(), total=len(df))]\n",
    "    \n",
    "    df[[\"target_data_clean\",\"bbox_list\"]] = df[\"target_data\"].apply(lambda x: pd.Series(transform_annotations(json.loads(x), defaults=keys,remove_bbox=True)))\n",
    "\n",
    "    # Ensure that all attributes are in the same order\n",
    "    df[\"target_data_clean\"] = df[\"target_data_clean\"].apply(order_json)\n",
    "    \n",
    "    # Generate Swift format examples\n",
    "    converted_data = df.apply(create_swift_example, axis=1).tolist()\n",
    "    \n",
    "    # Save converted dataset\n",
    "    output_path = os.path.join(dataset_dir, f\"conversations_{split_name}_swift_format.json\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(converted_data, f, indent=2)\n",
    "    \n",
    "    return output_path, df\n",
    "\n",
    "# Process all dataset splits\n",
    "swift_files = []\n",
    "swift_df_all = {}\n",
    "\n",
    "\n",
    "for split_name, dataset_split in dataset.items():\n",
    "    print(f\"Processing dataset: {split_name}\")\n",
    "    output_file, df = convert_dataset(dataset_split, split_name)\n",
    "    swift_files.append(output_file)\n",
    "    swift_df_all[split_name]=df\n",
    "    print(f\"Finished writing: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Store JSON Schema format for usage during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical concatenation\n",
    "merged = pd.concat([swift_df_all[\"test\"][\"target_data_clean\"], swift_df_all[\"train\"][\"target_data_clean\"], swift_df_all[\"dev\"][\"target_data_clean\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genson import SchemaBuilder\n",
    "\n",
    "# json_targets = swift_df_all[\"train\"][\"target_data_clean\"]\n",
    "json_targets = merged\n",
    "builder = SchemaBuilder()\n",
    "\n",
    "for item in json_targets:\n",
    "    builder.add_object(item)\n",
    "\n",
    "schema = builder.to_schema()\n",
    "\n",
    "outfile_json_schema = f\"{dataset_dir}/groundtruth_schema.json\"\n",
    "#write schema to file\n",
    "with open(outfile_json_schema, \"w\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "JSON(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "# Let's read and show a sample entry of the first dataset\n",
    "print(f\"dataset file: {swift_files[0]}\")\n",
    "data = json.load(open(swift_files[0]))\n",
    "JSON(data[4], expanded=False, root = \"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final target json format, to be generated by the LLM\n",
    "JSON(json.loads(data[4][\"messages\"][2][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync $dataset_dir $dataset_s3_uri --exclude \".ipynb_checkpoints/*\" --quiet\n",
    "print(f\"\\n✅ Dataset successfully uploaded to {dataset_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The processed dataset is now ready for training document understanding models with Swift. In the next notebook we will execute the training and consider the following points.\n",
    "\n",
    "1. **Model Selection**: Choose appropriate multimodal vision models (e.g., Qwen-VL, Yi-VL)\n",
    "2. **Training Configuration**: Set hyperparameters in Swift training scripts\n",
    "3. **Validation**: Use the processed dev set for training monitoring\n",
    "4. **Evaluation**: Utilize the test set for final model benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
