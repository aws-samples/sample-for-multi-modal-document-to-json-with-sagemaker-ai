{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5ea081",
   "metadata": {},
   "source": [
    "# Automated Evaluation of Document Understanding Models for Document to JSON\n",
    "\n",
    "This notebook provides a comprehensive framework for evaluating fine-tuned models that convert document PDFs/Images into structured JSON data. The evaluation includes:\n",
    "\n",
    "- **Multi-metric assessment** (Exact Match, Character Error Rate, ROUGE)\n",
    "- **Feature type analysis** (Value Extraction, Classification, Freeform Text)\n",
    "- **Interactive visualizations**\n",
    "- **Model performance comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b64bb7",
   "metadata": {},
   "source": [
    "# Setup & Configuration\n",
    "\n",
    "## Install Required Packages\n",
    "We use several specialized libraries for evaluation:\n",
    "- **Levenshtein**: For edit distance calculations\n",
    "- **plotnine**: For ggplot-style visualizations\n",
    "- **itables**: Interactive DataFrame display\n",
    "- **evaluate**: Standard NLP metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets==3.6.0 Levenshtein==0.26.1 plotnine==0.14.5 itables==2.2.4 regex==2024.11.6 evaluate==0.4.4 cer==1.2.0 rouge_score==0.1.2 seaborn sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049a108-3911-421d-9ed3-ffb4a3e67b4b",
   "metadata": {},
   "source": [
    "## Environment Initialization\n",
    "\n",
    "* Interactive widget configuration\n",
    "* Table display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169fa0c-1e93-4f68-8eed-221721334365",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization for interactive widgets and table display\n",
    "from itables import init_notebook_mode, show\n",
    "import itables.options as opt\n",
    "\n",
    "opt.columnDefs = [{\"width\": \"100px\", \"targets\": \"_all\"}]\n",
    "opt.column_filters = \"footer\"\n",
    "opt.showIndex = False\n",
    "# init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b82857",
   "metadata": {},
   "source": [
    "# Data Processing & Preparation\n",
    "\n",
    "* Load and parse inference outputs\n",
    "* Data cleaning and formatting\n",
    "\n",
    "## Retrieve Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7a33e-5e60-4c1d-a4a1-0c0c42aba867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5bb30-06c9-4092-bc3f-2f81ce9b6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from utils.helpers import download_and_extract\n",
    "import sagemaker\n",
    "\n",
    "default_bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "# Configuration for dataset location\n",
    "dataset_s3_uri = f\"s3://{default_bucket}/fatura2-train-data/\" #\"s3://your-bucket/dataset/\"\n",
    "dataset_base_dir = \"./data/processed/\"\n",
    "\n",
    "print(\"⬇️ Starting download of dataset...\")\n",
    "!aws s3 cp $dataset_s3_uri $dataset_base_dir --recursive --quiet\n",
    "\n",
    "results_dir = \"./data/results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Load list of inference runs\n",
    "# You can modify the csv manually to compare only specific runs or models\n",
    "inference_runs = pd.read_csv(\"./results_to_compare.csv\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "print(\"⬇️ Starting download of results...\")\n",
    "inference_runs = inference_runs.progress_apply(\n",
    "    download_and_extract,\n",
    "    axis=1,\n",
    "    output_dir=results_dir,\n",
    "    s3_client=s3_client,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e740131",
   "metadata": {},
   "source": [
    "## Parse Inference Results\n",
    "Handling potential JSON formatting issues and nested structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dbfc5-dd27-447b-aaa0-0533c7f28763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.evaluation import process_run\n",
    "\n",
    "results_df = []\n",
    "expected_num_rows = -1\n",
    "for run in tqdm(inference_runs.itertuples(), desc=\"Processing files\"):\n",
    "    result_files = run.results_files\n",
    "\n",
    "    for file in result_files:\n",
    "        result, num_rows = process_run(run, file, results_dir, expected_num_rows)\n",
    "        if result is not None:\n",
    "            results_df.append(result)\n",
    "            expected_num_rows = num_rows\n",
    "\n",
    "\n",
    "df_test = pd.concat(results_df)\n",
    "# show(df_test[[\"response\", \"response_raw\", \"label\", \"model\"]])\n",
    "df = results_df[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08fb9b3-7ccc-40e0-abb8-1055d9362c40",
   "metadata": {},
   "source": [
    "## View Inference Result Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89107851-6a98-4c63-b1b9-ff75633126f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "# Lets have a look how a sample inference result looks like\n",
    "JSON(df.iloc[2].to_dict(), root=\"inference sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb677c05-9deb-42c1-9de5-8ab623a9d913",
   "metadata": {},
   "source": [
    "## Show Diff for Inference and GroundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a442dd2-9e55-4f57-82a3-d1ea072cdb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.docdiff import get_pil_image\n",
    "\n",
    "# filer_single_model=inference_runs.iloc[-1][\"human_name\"]\n",
    "# print(f\"We filter for model: {filer_single_model}\")\n",
    "# df_test=df_test[df_test[\"pretty_name\"]==filer_single_model]\n",
    "\n",
    "df_test[\"imagePILL\"] = df_test[:10].images.map(\n",
    "    lambda x: get_pil_image(x[0], dataset_base_dir)\n",
    ")\n",
    "df_final = df_test[[\"imagePILL\", \"response\", \"labels\"]]\n",
    "# df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb585ea-6feb-4eb9-a1ec-5661cd7363ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from utils.docdiff import image_formatter, get_diff\n",
    "\n",
    "df_final[\"diff\"] = df_final.apply(get_diff, axis=1)\n",
    "df_final2 = df_final[[\"imagePILL\", \"diff\"]][0:10]\n",
    "\n",
    "HTML(df_final2.to_html(formatters={\"imagePILL\": image_formatter}, escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cdfa2-3b2f-4a6e-8f4f-842b9cd718a2",
   "metadata": {},
   "source": [
    "## Flatten JSON and Calculate Edit Distance per Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec727475-82ea-44e2-ae78-db715238f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dist import add_edit_distance\n",
    "# we need to know what attributes are in the destination data structure\n",
    "columns = df_test[\"labels\"].apply(set).apply(list).explode().drop_duplicates().tolist()\n",
    "# text_propetry_name = \"text\"\n",
    "text_propetry_name = None # if your results only contain the text for each key without a nested structure\n",
    "df_dist = add_edit_distance(df_test, columns, text_propetry_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b781920-17d4-4b63-a084-b297e2edf6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit_cols = list(set(df_dist[\"labels\"].values[0])) # NOTE: we only take the entities from the first label assuming it is complete\n",
    "melted_df = pd.melt(\n",
    "    df_dist,\n",
    "    id_vars=df_test.columns,\n",
    "    value_vars=columns,\n",
    "    var_name=\"entity\",\n",
    "    value_name=\"dist\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066d8ca-e232-44fb-a909-84d2acc582e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dist import unfold\n",
    "df_unfold = pd.DataFrame(list(melted_df.apply(unfold, axis=1)))\n",
    "\n",
    "df_multi = pd.concat([melted_df, df_unfold], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d96ecf-fa16-405a-8a36-fd3574bc950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = list(set(df[\"model\"].values))[0]\n",
    "df_single = df_multi[df_multi[\"model\"] == model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee321da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the grountruth outputs for each entity\n",
    "main_columns = [\"fileid\",\"entity\",\"dist\",\"response_val\",\"label_val\",\"model\",\"model_name\"]\n",
    "\n",
    "show(df_single[main_columns][:100],\n",
    "     classes=['display', 'compact'],\n",
    "     columnDefs=[\n",
    "         {\"width\": \"80px\", \"targets\": \"_all\"},\n",
    "         {\"className\": \"dt-center\", \"targets\": \"_all\"}\n",
    "     ],\n",
    "     dom='Bfrtip',\n",
    "     scrollX=False,\n",
    "     lengthMenu=[5, 10, 25, 50],\n",
    "     pageLength=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ec289",
   "metadata": {},
   "source": [
    "# Feature Type Analysis and Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca372f4e-0225-442b-8aad-ee426f2d24d2",
   "metadata": {},
   "source": [
    "## Analyse Feature Distribution of Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dbf19f-02cc-4e05-a505-b978a146dae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.entities import analyze_string_distribution_by_entity\n",
    "# Usage\n",
    "results_df = analyze_string_distribution_by_entity(df_single, col_value=\"label_val\")\n",
    "\n",
    "# show(results_df,\n",
    "#      classes=['display', 'compact'],\n",
    "#      columnDefs=[\n",
    "#          {\"width\": \"150px\", \"targets\": \"_all\"},\n",
    "#          {\"className\": \"dt-center\", \"targets\": \"_all\"}\n",
    "#      ],\n",
    "#      dom='Bfrtip',\n",
    "#      scrollX=True,\n",
    "#      lengthMenu=[5, 10, 25, 50],\n",
    "#      pageLength=10\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42eeecd-2b62-470e-a631-ea589c7413b2",
   "metadata": {},
   "source": [
    "## Show Entities by Missing GroundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53dca57-3f17-4599-a4ad-ab966ff5e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_bar, theme_minimal, theme_bw,labs, theme, element_text, coord_flip\n",
    "\n",
    "# Sort the filtered dataframe by metric_value in descending order\n",
    "plot_data = (results_df[results_df['metric_name'] == 'null_percentage']\n",
    "             .sort_values('metric_value', ascending=True))  # ascending=True because coord_flip will reverse it\n",
    "\n",
    "# Create ordered factor for entity based on metric_value\n",
    "plot_data['entity'] = pd.Categorical(plot_data['entity'], \n",
    "                                   categories=plot_data['entity'].tolist(),\n",
    "                                   ordered=True)\n",
    "\n",
    "(ggplot(plot_data, \n",
    "        aes(x='entity', y='metric_value')) +\n",
    " geom_bar(stat='identity', fill='steelblue') +\n",
    " coord_flip() +\n",
    " labs(title='Null Percentage by Entity',\n",
    "      x='Entity',\n",
    "      y='Null Percentage') +\n",
    " theme_bw())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77698ed-cd1d-4c58-846c-e5332e728eb5",
   "metadata": {},
   "source": [
    "We can observe in the plot above, that there are multiple entites, which have more than 60% missing ground truth. GST(20%) has for example 90% missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446a13a-3408-4908-81ae-a68e847c7c2a",
   "metadata": {},
   "source": [
    "## Entity String Length Analysis with Boxplots\n",
    "\n",
    "Lets analyze and categorize entities based on their string lengths using boxplots for visualization.\n",
    "\n",
    "**Purpose**\n",
    "- Understand the distribution of entity string lengths\n",
    "- Classify entities into different categories based on their characteristics\n",
    "- Establish thresholds for entity classification\n",
    "\n",
    "**Feature Categorization**\n",
    "\n",
    "We categorize the entities into three main types:\n",
    "1. **Missing Ground Truth**: Entities with high null percentage\n",
    "2. **Short Text**: Entities with shorter length (e.g., dates, amounts)\n",
    "3. **Long Text**: Entities with longer, free-form text\n",
    "\n",
    "**Thresholds**\n",
    "- `null_percentage`: Threshold for identifying missing data (default: 70%)\n",
    "- `length_mean`: Threshold for distinguishing between short and long text (default: 50 characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741097c6-055c-47a9-a84c-2bef1ccb178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.entities import plot_length_boxplots_with_limits_horizontal\n",
    "\n",
    "stats_df = plot_length_boxplots_with_limits_horizontal(df_single, col_value=\"label_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427daa14-5674-4cd1-b5b6-8a9546c0e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.entities import categorize_features\n",
    "#??categorize_features\n",
    "feature_categories = categorize_features(results_df, null_percentage_threshold=70, length_mean_threshold=50)\n",
    "\n",
    "df = pd.DataFrame.from_dict(feature_categories, orient='index', columns=['category'])\n",
    "grouped = df.groupby('category')\n",
    "for name, group in grouped:\n",
    "    print(f\"\\nGroup: {name}\")\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f14168-4e28-4e35-be44-27c04d709267",
   "metadata": {},
   "source": [
    "Next let's display the distribution and values of the features for visual analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db73aa-c3d0-4d36-bf6d-1f759365e19b",
   "metadata": {},
   "source": [
    "# Performance Evaluation for Entities using Edit Distance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381fcdf-86fc-4355-aeb7-311037ea8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64acf76e-e9f9-446a-b3b3-bec125b92118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dist_cut\"] = df[\"dist\"].clip(upper=6)\n",
    "df[\"dist_cut\"] = df[\"dist_cut\"].replace(-2, 7)\n",
    "df[\"dist_cut\"] = df[\"dist_cut\"].replace(-3, 8)\n",
    "df[\"dist_cut\"] = df[\"dist_cut\"].replace(-4, 9)\n",
    "fact_name = sorted(list(set(df[\"dist_cut\"].values)))\n",
    "mapper = {\n",
    "    -1: \"missing groundtruth\",\n",
    "    0: \"exact match\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6+\",\n",
    "    7: \"predicted 'None'\",\n",
    "    8: \"key missing\",\n",
    "    9: \"invalid JSON\"\n",
    "}\n",
    "rev_map = {value: key for key, value in mapper.items()}\n",
    "df[\"dist_cut\"] = df[\"dist_cut\"].replace(mapper)\n",
    "df[\"dist_cut\"] = pd.Categorical(\n",
    "    df[\"dist_cut\"], categories=mapper.values(), ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc98bac-676b-4834-9c8f-b71cc6d72366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'entity_name' based on the dictionary\n",
    "df[\"entity_type\"] = df[\"entity\"].map(feature_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8afb20-064a-43d6-8d8c-7c9779e556d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values([\"entity_type\", \"entity\"])\n",
    "\n",
    "df_entities = df_sorted[[\"entity\", \"entity_type\"]].drop_duplicates(subset=[\"entity\"])\n",
    "df_entities.reset_index(inplace=True)\n",
    "\n",
    "ordered_values = list(df_entities[\"entity\"])\n",
    "change_indices = df_entities.index[\n",
    "    df_entities[\"entity_type\"] != df_entities[\"entity_type\"].shift()\n",
    "].tolist()\n",
    "\n",
    "\n",
    "# Reorder categories in the dataframe\n",
    "df_sorted['pretty_name_ordered'] = pd.Categorical(df_sorted['pretty_name'], categories=list(set(df_sorted['pretty_name'].values)), ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a32cfd-6f94-40a5-bbd0-cda140800fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from textwrap import fill\n",
    "\n",
    "def plot_bar_chart_all_entities(df, ordered_values, width=22, height=18, geom_vlines=[2.5, 4.5, 8.5, 10.5]):\n",
    "\n",
    "    custom_colors = [\"#bababa\",\"#66c2a5\",\"#abdda4\",\"#e6f598\",\"#ffffbf\",\"#fee08b\",\"#fdae61\",\"#f46d43\",\"#abd9e9\",\"#74add1\",\"#bebada\"]\n",
    "\n",
    "    def wrap_labels(label):\n",
    "        return fill(label, width=width+4)  # Wrap text at 15 characters per line\n",
    "    \n",
    "    # Create the stacked bar chart\n",
    "    return (\n",
    "        ggplot(df, aes(x=\"entity\", fill=\"factor(dist_cut)\"))\n",
    "        + geom_bar(position=\"stack\", color=\"black\")\n",
    "        + facet_wrap(\" ~pretty_name_ordered\", scales=\"free\", labeller=lambda x: wrap_labels(x))\n",
    "        + labs(x=\"Entity\", y=\"Count\", fill=\"Char. edit distance\")\n",
    "        + coord_flip()\n",
    "        + scale_fill_manual(values=custom_colors, labels=mapper)\n",
    "        + theme_minimal()\n",
    "        + theme(figure_size=(width, height))\n",
    "        + geom_vline(\n",
    "            xintercept=geom_vlines, linetype=\"dashed\", color=\"#4d4d4d\", size=2.2\n",
    "        )\n",
    "        + scale_x_discrete(limits=ordered_values)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ea1f4-6689-49de-8692-87a9847878e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff53ad-cb19-40ef-97f9-40a8ac6e797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to draw lines in the graphs to distinguish the different entity types\n",
    "feature_class_lines = [index + 0.5 for index in change_indices[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ac389-08b8-4579-8bd8-43b7fd125eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21972a9f-85f0-4a4b-a8a1-bf27a9d542f6",
   "metadata": {},
   "source": [
    "Note: You might need to adjust the width and height below to find a ratio that shows the graphs correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f62d4-38ae-4ea3-9b7f-d734106ea9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_bar_chart_all_entities(\n",
    "    df_sorted, ordered_values, width=15, height=8, geom_vlines=feature_class_lines\n",
    ")\n",
    "display(Markdown(f\"### All Entities\"))\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e739e30-e538-4017-9b7d-39bd1ca9ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.entities import FeatureCategory\n",
    "feature_to_remove = FeatureCategory.MISSING_GROUND_TRUTH\n",
    "remaining_columns = df_entities[df_entities[\"entity_type\"] != feature_to_remove]\n",
    "remaining_columns.reset_index(inplace=True)\n",
    "\n",
    "ordered_values = list(remaining_columns[\"entity\"])\n",
    "\n",
    "df_filtered = df_sorted[df_sorted[\"entity\"].isin(ordered_values)]\n",
    "\n",
    "change_indices = remaining_columns.index[\n",
    "    remaining_columns[\"entity_type\"] != remaining_columns[\"entity_type\"].shift()\n",
    "].tolist()\n",
    "feature_class_lines = [index + 0.5 for index in change_indices[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2c422-21a3-4e3a-940e-d8349a2edd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_bar_chart_all_entities(\n",
    "    df_filtered, ordered_values, width=15, height=8, geom_vlines=feature_class_lines\n",
    ")\n",
    "display(Markdown(f\"### Relevant Entities\"))\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8867b-a20e-4faf-b92e-4441efc94a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot.save('./images/fatura2-evaluation-heatmap-char-edit-distance.png', dpi=300, facecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b05f9-0e35-4781-b909-42c82153cd67",
   "metadata": {},
   "source": [
    "# Model Performance Comparison\n",
    "## Calculate Metrics for Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff1511-6b7d-4ba8-b6f1-a7b842f27fe8",
   "metadata": {},
   "source": [
    "Note: If you get an error message `AttributeError: 'DownloadConfig' object has no attribute 'token'` in the cell below restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef847ce8-5077-49ac-9b10-2f89c89a57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup metrics\n",
    "import evaluate\n",
    "from cer import calculate_cer\n",
    "\n",
    "# eval_metrics = [\"exact_match\", \"character\"]\n",
    "eval_metrics = [\"exact_match\", \"character\", \"bleu\", \"rouge\"]\n",
    "evaluations = [evaluate.load(metric) for metric in eval_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd5133-3c9f-47af-a3c1-c3770f9bbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(references, predictions):\n",
    "    metrics = []\n",
    "    for metric in evaluations:\n",
    "        res = metric.compute(predictions=predictions, references=references)\n",
    "        metrics.append(res)\n",
    "\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        for key, value in metric.items():\n",
    "            results[key] = value\n",
    "    return results\n",
    "\n",
    "\n",
    "df_eval = df_filtered.groupby([\"model\", \"model_name\", \"pretty_name\", \"entity\"]).apply(\n",
    "    lambda x: calculate_metrics(\n",
    "        x[\"label_val\"].astype(str).values, x[\"response_val\"].astype(str).values\n",
    "    )\n",
    ")\n",
    "\n",
    "df_eval = df_eval.reset_index()\n",
    "# Convert the dictionary column into separate columns\n",
    "df_eval = pd.concat([df_eval, df_eval[0].apply(pd.Series)], axis=1)\n",
    "df_eval = df_eval.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d494c5a-3430-4034-ac47-71cf7059ae97",
   "metadata": {},
   "source": [
    "## Exact Match Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce28964-9449-40ee-9b40-31d873303337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa168f5-edde-454e-b114-3a2a732aabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "# Create the stacked bar chart\n",
    "plot = (\n",
    "    ggplot(df_eval, aes(x=\"entity\", y=\"exact_match\", group=\"model\", fill=\"pretty_name\"))\n",
    "    + geom_bar(stat=\"identity\", position=\"dodge\", colour=\"gray\")\n",
    "    + labs(\n",
    "        title=\"Exact match (higher better)\",\n",
    "        x=\"Entity\",\n",
    "        y=\"exact_match\",\n",
    "        fill=\"model_name\",\n",
    "    )\n",
    "    + coord_flip()\n",
    "    # + scale_fill_manual(values=custom_colors, labels=fact_name)\n",
    "    + scale_fill_brewer(type=\"qual\", palette=\"Set3\")\n",
    "    + theme_minimal()\n",
    "    + theme(figure_size=(12, 8))\n",
    "    + theme(axis_text_x=element_text(angle=45, hjust=1))\n",
    "    + scale_y_continuous(\n",
    "        breaks=[x / 100.0 for x in range(0, 101, 5)],\n",
    "        labels=[f\"{i}%\" for i in range(0, 101, 5)],\n",
    "    )\n",
    "    + geom_hline(\n",
    "        yintercept=[x / 100.0 for x in range(0, 101, 5)],\n",
    "        color=\"darkgray\",\n",
    "        size=0.5,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    + geom_vline(\n",
    "        xintercept=feature_class_lines, linetype=\"dashed\", color=\"#4d4d4d\", size=2.2\n",
    "    )\n",
    "    + scale_x_discrete(limits=ordered_values)  # ordering\n",
    ")\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be399298-d8c1-40be-9e2e-9905e72d2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot.save('./images/fatura2-evaluation-exact-match.png', dpi=300, facecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a6ad3d-5261-4898-8f55-1359926cf82b",
   "metadata": {},
   "source": [
    "## Character Error Correction - cer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695066e3-ad19-444a-b5f4-9c297fde4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import *\n",
    "\n",
    "# Create the stacked bar chart\n",
    "(\n",
    "    ggplot(df_eval, aes(x=\"entity\", y=\"cer_score\", group=\"model\", fill=\"pretty_name\"))\n",
    "    + geom_bar(stat=\"identity\", position=\"dodge\", color=\"gray\")\n",
    "    + labs(\n",
    "        title=\"cer_score - Character edits for entities (lower better)\",\n",
    "        fill=\"model_name\",\n",
    "    )\n",
    "    + coord_flip()\n",
    "    # + scale_fill_manual(values=custom_colors, labels=fact_name)\n",
    "    + scale_fill_brewer(type=\"qual\", palette=\"Set3\")\n",
    "    + theme_minimal()\n",
    "    + theme(figure_size=(12, 8))\n",
    "    + theme(axis_text_x=element_text(angle=45, hjust=1))\n",
    "    + geom_vline(\n",
    "        xintercept=feature_class_lines, linetype=\"dashed\", color=\"#4d4d4d\", size=2.2\n",
    "    )\n",
    "    + scale_x_discrete(limits=ordered_values)  # ordering\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c2d84-aa2c-4309-8904-8ac9b02735fb",
   "metadata": {},
   "source": [
    "## Aggregate Model Metrics in Table View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f1d59-bd80-48d0-ad93-e33e7cd87760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract exact match entities\n",
    "exact_match_feature = FeatureCategory.SHORT_TEXT\n",
    "exact_match_columns = df_entities[df_entities[\"entity_type\"] == exact_match_feature]\n",
    "\n",
    "exact_match_entities = list(exact_match_columns[\"entity\"])\n",
    "exact_match_df = df_eval[df_eval[\"entity\"].isin(exact_match_entities)]\n",
    "\n",
    "\n",
    "# Calculate accuracy for each model as the mean of exact match scores\n",
    "exact_aggregate = (\n",
    "    exact_match_df.groupby([\"model\", \"model_name\", \"pretty_name\"])\n",
    "    .agg({\"exact_match\": \"mean\",\"cer_score\":\"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "exact_aggregate.columns = [\"model\", \"model_name\", \"pretty_name\",\"accuracy (exact match)\",\"cer_score\"]\n",
    "exact_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96decc84-7aff-443d-b5c7-f88b1dc67e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the entities of type \"ngram\"\n",
    "summary_feature = FeatureCategory.LONG_TEXT\n",
    "summary_columns = df_entities[df_entities[\"entity_type\"] == summary_feature]\n",
    "ngram_entities = list(summary_columns[\"entity\"])\n",
    "\n",
    "# Filter the DataFrame for ngram entities\n",
    "ngram_df = df_eval[df_eval[\"entity\"].isin(ngram_entities)]\n",
    "\n",
    "# Group by 'model' and 'model_name' and calculate the mean of the ROUGE scores for ngram entities\n",
    "ngram_rouge_scores = (\n",
    "    ngram_df.groupby([\"model\",\"pretty_name\"])\n",
    "    .agg({\"rouge1\": \"mean\", \"rouge2\": \"mean\", \"rougeL\": \"mean\", \"rougeLsum\": \"mean\"})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f882d9-f503-4666-9232-e9808a204efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the ROUGE score DataFrame with the accuracy DataFrame on 'model' and 'model_name'\n",
    "aggregated_metrics = ngram_rouge_scores.merge(\n",
    "    exact_aggregate, on=[\"model\",\"pretty_name\"], how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8e40a-9960-4295-becc-9dee88e20c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_columns = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\", \"accuracy (exact match)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c6f26-facb-43e9-9745-613b3f107915",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_metrics[metric_columns] = aggregated_metrics[metric_columns].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4769d3-11b2-4764-9202-e251de06e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_metrics[\"model\"] = aggregated_metrics[\"model\"].apply(\n",
    "    lambda model: model.replace(\"\\n\", \" \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554314be-be80-4a12-88fe-d36cbca7e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "def highlight_min(s, props=''):\n",
    "    return np.where(s == np.nanmin(s.values), props, '')\n",
    "    \n",
    "aggregated_metrics.style.apply(highlight_max, props='background-color:lightblue', axis=0)\n",
    "\n",
    "slice_ = ['rouge1','rouge2','rougeL','rougeLsum', 'accuracy (exact match)']\n",
    "aggregated_metrics.style.apply(highlight_max, props='background-color:#99d594;', axis=0, subset=slice_)\\\n",
    "         .apply(highlight_min, props='background-color:#99d594;', axis=0, subset=['cer_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d202d-8329-4842-9231-cf0f0cb51e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# markdown_table = aggregated_metrics.to_markdown()\n",
    "\n",
    "# # Print the markdown table\n",
    "# print(markdown_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65d5e3-18dc-40f9-9165-7d4c851f9287",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We analyzed the inference results for our invoice dataset. Then we visualized the results for each entity in an edit distance heatmap. Finally, we aggregated multiple metrics and compared the final metrics across different runs and models.\n",
    "\n",
    "## Main learnings:\n",
    "\n",
    "Constrained Decoding works, but the model can sometimes get into a recursive loop and exceed the max_token parameter during generation. The resulting output might thus not be a valid JSON. \n",
    "We perform the following steps to improve the results and resolve the invalid JSON challenges:\n",
    "\n",
    "* We removed fields with poor data quality, like the `Other` field in this dataset.\n",
    "* We made sure that each key of the unified set of target JSON keys is contained in the target JSON to be learned by the model. In cases with no ground truth, None is used as prediction value.\n",
    "* We made sure that the keys have the same ordering in the target JSON.\n",
    "* We added the expected target keys as a list to the prompt for training.\n",
    "\n",
    "These methods together allow us to achieve 98% accuracy (exact match) with a small Qwen2.5 VL 3B model and 300 used samples for training.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "You can choose the best model for your use case and proceed with the [deployment of the selected model](06_deploy_model_endpoint.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f922d-f55c-4679-bae8-2a10c653faf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbf525-087f-4ed2-b7c5-afaf72d93e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
