{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60500b71-7985-4149-941b-94449fcf3d87",
   "metadata": {},
   "source": [
    "# Deploying a Fine-tuned Vision Language Model to Amazon SageMaker AI for Inference\n",
    "\n",
    "This notebook guides you through the process of deploying a fine-tuned Vision Language Model (VLM) to Amazon SageMaker. The deployment process includes several key steps:\n",
    "\n",
    "1. **Environment Setup**: Installing necessary dependencies and configuring the AWS environment\n",
    "2. **Model Artifact Management**: Locating and selecting the fine-tuned model artifacts from S3\n",
    "3. **Container Infrastructure**: Building and pushing a custom container to Amazon ECR\n",
    "4. **SageMaker Deployment**: Creating and deploying a SageMaker endpoint\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- AWS credentials configured with appropriate permissions\n",
    "- AWS CLI installed\n",
    "- Access to the S3 bucket containing your model artifacts\n",
    "\n",
    "**Important Notes**\n",
    "\n",
    "- The deployment uses an ml.g5.2xlarge instance which provides GPU acceleration necessary for efficient inference\n",
    "- The custom container includes SWIFT framework for model serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a26393-f759-42fb-987a-5cf84019c613",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, we'll install `jq`, a lightweight command-line JSON processor. This will be used to parse AWS metadata and credentials later in our deployment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1c3dd-bc7a-4915-8f42-4300cba86dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -qq -y jq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248133b-287a-499e-8d87-82c423bc9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 sagemaker pandas huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99831e-0ee6-43fa-a448-da9850ff4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68975c5a-8e26-4226-99d3-020231c2b174",
   "metadata": {},
   "source": [
    "Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae9a63-9628-4a58-ab72-2a6b2a95b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sagemaker.model import Model\n",
    "from utils.model_manager import list_available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18a783-b392-420d-a04e-8bda4618bd51",
   "metadata": {},
   "source": [
    "## Initialize AWS Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44147bba-d2e6-4677-a1f6-8b9a2ffeb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session(boto_session=boto3.Session(region_name=region))\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be627f4-ab5a-4735-bbd4-0a02be46911d",
   "metadata": {},
   "source": [
    "## List Available Model Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c699474-bc2c-4309-9a1d-503c2214cd94",
   "metadata": {},
   "source": [
    "List the available models based on S3 key prefix and \"model.tar.gz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee4d05-b41e-412e-a2e5-064d09f4f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"finetune-\"\n",
    "\n",
    "df = list_available_models(bucket_name, model_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c3fd3-0b88-4ed5-bb53-25b9fe243b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the S3 URI from which we will download the model\n",
    "model_key=df['Key'].iloc[0]\n",
    "s3_model_uri = f\"s3://{bucket_name}/{model_key}\"\n",
    "print(f\"Selected model for deployment: {model_key}\")\n",
    "print(f\"S3 Model URI: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c2184-cfdc-4294-a5ad-5db89b61e0ba",
   "metadata": {},
   "source": [
    "## Build and Push Docker Inference Container\n",
    "\n",
    "Amazon SageMaker AI offers [three primary methods](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html) for deploying ML models to an SageMaker AI Inference Endpoint:\n",
    "1. Using pre-built SageMaker containers for standard frameworks like PyTorch or TensorFlow\n",
    "2. Modifying existing Docker containers with your own dependencies through a requirements.txt file\n",
    "3. Or creating completely custom containers that implements a web server listening for requests (/invocations) for maximum flexibility and control over dependencies and requirements.\n",
    "\n",
    "To run the fine-tuned models you will build our custom container and push it to Amazon Elastic Container Registry (ECR). The container includes:\n",
    "- SWIFT framework for model serving\n",
    "- VLLM for high-throughput and memory efficient inference\n",
    "- Required dependencies for the Qwen2-VL model\n",
    "- API endpoint modifications for SageMaker integration\n",
    "\n",
    "\n",
    "The container will be built from our Dockerfile and pushed to ECR, making it available for SageMaker to use when deploying our endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ea352-6b31-41dc-bb37-52f647346e21",
   "metadata": {},
   "source": [
    "### Docker Installation\n",
    "\n",
    "To create our custom container for model serving, we first need Docker installed in our environment. This script handles the installation of Docker and its dependencies, including necessary security keys and repository configurations.\n",
    "\n",
    "**Install docker-cli**\n",
    "\n",
    "At the end of this install you should see,\n",
    "\n",
    "```bash\n",
    "Client: Docker Engine - Community\n",
    " Version:           20.10.24\n",
    " API version:       1.41\n",
    " Go version:        go1.19.7\n",
    " Git commit:        297e128\n",
    " Built:             Tue Apr  4 18:21:03 2023\n",
    " OS/Arch:           linux/amd64\n",
    " Context:           default\n",
    " Experimental:      true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d7d4f-9cbd-46a3-9cc7-05634c4a0efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash docker-artifacts/01_docker_install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca114b-fb6a-4b83-9474-2f9d4bd1530e",
   "metadata": {},
   "source": [
    "### Build and push a custom image\n",
    "\n",
    "SageMaker Inference supports simplified deployment of Qwen2 using Large Model Inference (LMI) container images as indicated [here](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) and available images listed [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).\n",
    "\n",
    "However, video inference is not a supported by vLLM/OpenAI api server out of the box and so we design a custom image with a custom inference handler adapter from vllm's [api_server.py](https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py) to include video inference tag. We can build a custom image inside SageMaker Studio using `docker-cli`.\n",
    "\n",
    "**Build our custom Docker image containing custom inference handler and push it to Amazon ECR (Elastic Container Registry).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538728f4-4c44-497a-a6aa-40e04857ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = \"swift-json-vlm-container-finetuned\"\n",
    "os.environ['REPO_NAME'] = REPO_NAME\n",
    "os.environ[\"S3_MODEL_URI\"]=s3_model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee339f-3284-4f85-8862-c06759e10551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash -s {region} {account_id}\n",
    "\n",
    "REGION=$1\n",
    "\n",
    "VERSION_TAG=\"latest\"\n",
    "CURRENT_ACCOUNT_NUMBER=$2\n",
    "\n",
    "echo \"bash 02_build_and_push.sh $REPO_NAME $VERSION_TAG $REGION $CURRENT_ACCOUNT_NUMBER\"\n",
    "cd docker-artifacts && bash 02_build_and_push.sh $REPO_NAME $VERSION_TAG $REGION $CURRENT_ACCOUNT_NUMBER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da3271-aa4b-48b7-8d83-1129048357f3",
   "metadata": {},
   "source": [
    "### Getting Container Image URI\n",
    "\n",
    "Retrieve the full URI of our Docker image from ECR. This URI is essential for SageMaker deployment as it tells SageMaker exactly where to find our custom container image. The URI follows the format:\n",
    "`{account_id}.dkr.ecr.{region}.amazonaws.com/{repository_name}:{tag}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ce6f6-ac3c-4b12-87b6-647f800edf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{REPO_NAME}:latest\"\n",
    "print(f\"Base image to deploy a SageMaker endpoint: {image_uri}\")\n",
    "\n",
    "os.environ['CUSTOM_IMAGE'] = image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fe5ca-226a-4737-a451-399fbc709337",
   "metadata": {},
   "source": [
    "## Understanding the Model Serving Architecture\n",
    "\n",
    "When we deploy our model to a SageMaker endpoint, here's how the components work together:\n",
    "\n",
    "1. **Docker Container Structure**:\n",
    "   - The container runs on the SageMaker instance (ml.g5.2xlarge)\n",
    "   - The `swift deploy` command in our ENTRYPOINT starts a server that:\n",
    "     - Loads the model into GPU memory\n",
    "     - Sets up API endpoints for inference\n",
    "     - Listens on port 8080 for requests\n",
    "\n",
    "2. **Request Flow**:\n",
    "   - External requests → SageMaker endpoint → Container's port 8080\n",
    "   - The `sed` commands we used modified the API paths to match SageMaker's expected structure:\n",
    "     - `/ping` for health checks\n",
    "     - `/invocations` for model inference\n",
    "     - `/invocations/completions` for completion requests\n",
    "\n",
    "3. **SWIFT Framework's Role**:\n",
    "   - Handles model loading and optimization with vLLM\n",
    "   - Manages inference requests\n",
    "   - Provides OpenAI-compatible API interface\n",
    "   - Handles input preprocessing and output formatting\n",
    "\n",
    "4. **SageMaker Integration**:\n",
    "   - Routes HTTPS requests to our container\n",
    "   - Manages container lifecycle\n",
    "   - Handles authentication and scaling\n",
    "   - Monitors container health via the `/ping` endpoint\n",
    "\n",
    "This setup allows us to serve our fine-tuned model with production-grade reliability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1f1e5-1789-4218-83ce-af384449cff3",
   "metadata": {},
   "source": [
    "**[Optional] We can run our container interactively in Terminal by using the command below. Make sure you are using a GPU instance for your Jupyterlab space since SWIFT inference requires a GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f6cf4-ae0d-4b7a-a302-ca733f64c3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%writefile run_container.sh\n",
    "# # Get credentials from instance metadata\n",
    "# export $(curl -s 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI | jq -r '\"AWS_ACCESS_KEY_ID=\"+.AccessKeyId, \"AWS_SECRET_ACCESS_KEY=\"+.SecretAccessKey, \"AWS_SESSION_TOKEN=\"+.Token')\n",
    "\n",
    "# # Now run your docker container with these environment variables\n",
    "# # Add --entrypoint /bin/bash in case you want to manually look into the container\n",
    "# set -x\n",
    "# docker run  --gpus all --network sagemaker -it -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN \\\n",
    "#  -e ADAPTER_URI=$S3_MODEL_URI $REPO_NAME\n",
    "\n",
    "# # docker run --network sagemaker -it -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN \\\n",
    "# #  -e ADAPTER_URI=$S3_MODEL_URI $REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4a30e-6fa5-48a5-8c3a-be58f8821527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !bash run_container.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d439b7e-321b-428f-a6c0-53b7fa821acc",
   "metadata": {},
   "source": [
    "## Creating a SageMaker Model and deploy a SageMaker endpoint\n",
    "\n",
    "Finally, we'll create a SageMaker model and deploy it to an inference endpoint. This will give us an HTTPS endpoint that we can use for inference.\n",
    "\n",
    "Note: We're using an ml.g5.2xlarge instance which provides GPU acceleration necessary for efficient inference with a small multimodal model.\n",
    "\n",
    "For more throughphut, lower latency, or when deploying a bigger model you might want to use a bigger instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7045db-ada0-4152-84b6-701b3e5495b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"document-to-json\"\n",
    "sm_endpoint_name = \"document-to-json\"\n",
    "\n",
    "print(f\"Model name: {sm_model_name}\")\n",
    "print(f\"Endpoint name: {sm_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95428db3-ce57-4068-9c29-6c3d871ce3c9",
   "metadata": {},
   "source": [
    "Deploy our model to a SageMaker endpoint using an ml.g5.2xlarge instance. This GPU-enabled instance type provides the computational power needed for efficient inference with a Qwen2-VL model. The deployment:\n",
    "- Creates a SageMaker model using our custom container\n",
    "- Configures the endpoint with specified resources\n",
    "- Initiates asynchronous deployment (wait=False)\n",
    "- Sets up HTTPS endpoint for inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074d5a3-41ba-4431-a01b-6c901f102fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main deployment logic\n",
    "from utils.helpers import check_model_exists, check_endpoint_config_exists, check_endpoint_exists, delete_all_resources\n",
    "\n",
    "endpoint_exists = check_endpoint_exists(endpoint_name=sm_endpoint_name, sm_client=sm_client)\n",
    "model_exists = check_model_exists(sm_model_name, sm_client=sm_client)\n",
    "config_exists = check_endpoint_config_exists(sm_endpoint_name, sm_client=sm_client)\n",
    "\n",
    "if endpoint_exists or model_exists or config_exists:\n",
    "    print(f\"\\nFound existing resources:\")\n",
    "    if endpoint_exists:\n",
    "        print(f\"- Endpoint: {sm_endpoint_name}\")\n",
    "    if model_exists:\n",
    "        print(f\"- Model: {sm_model_name}\")\n",
    "    if config_exists:\n",
    "        print(f\"- Endpoint config: {sm_endpoint_name}\")\n",
    "    \n",
    "    delete_all_resources(sm_model_name, sm_endpoint_name, sm_client=sm_client)\n",
    "\n",
    "# Define environment variables for the model\n",
    "environment = {\n",
    "    \"USE_HF_TRANSFER\": \"true\",  # Enable faster downloads\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n",
    "    \"SIZE_FACTOR\": \"8\",\n",
    "    \"MAX_PIXELS\": \"602112\",\n",
    "    \"ADAPTER_URI\": s3_model_uri,\n",
    "    \"SM_VLLM_SERVED_MODEL_NAME\": sm_model_name, # you can name your model whatever you want\n",
    "    \"SM_VLLM_LIMIT_MM_PER_PROMPT\": \"image=2, video=0\", # max number of images allowed in prompt. Increase for multi-page documents. Requires more memory.\n",
    "    \"SM_VLLM_MAX_NUM_SEQS\":\"8\", # decrease if less GPU memory available\n",
    "    \"SM_VLLM_MAX_MODEL_LEN\":\"38608\", # max context length, decrease if less GPU memory available\n",
    "    \"SM_VLLM_DTYPE\": \"bfloat16\"\n",
    "}\n",
    "\n",
    "# If we get here, either nothing existed or we've cleaned up\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    name=sm_model_name,\n",
    "    env=environment,\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint is now being deployed.... This may take several minutes.\")\n",
    "\n",
    "# Deploy a new endpoint\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name=sm_endpoint_name,\n",
    "    wait=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff834f58-f5ea-4c9c-8d43-b3412b863b24",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "After deploying the model as a SageMaker endpoint, we can call the model endpoint to run inference with the sample code in the next notebook [07_consume_model.ipynb](./07_consume_model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa922a2-7453-4786-a336-3728239aef95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
