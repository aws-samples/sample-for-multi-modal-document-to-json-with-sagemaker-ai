{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed10533c",
   "metadata": {},
   "source": [
    "# Converting Fatura2 Dataset to Hugging Face Format\n",
    "\n",
    "This notebook demonstrates how to convert the Fatura2 invoice dataset into a Hugging Face dataset format for easier use in document processing tasks. The Fatura2 dataset contains invoice images paired with JSON annotations.\n",
    "\n",
    "\n",
    "Reference: Fatura Dataset on Zenodo\n",
    "\n",
    "Paper: Limam, M., Dhiaf, M., & Kessentini, Y. (2023). FATURA: A Multi-Layout Invoice Image Dataset for Document Analysis and Understanding - [https://arxiv.org/abs/2311.11856](https://arxiv.org/abs/2311.11856)\n",
    "\n",
    "Dataset: Limam, M., Dhiaf, M., & Kessentini, Y. (2023). FATURA Dataset. Zenodo. [https://doi.org/10.5281/zenodo.10371464](https://doi.org/10.5281/zenodo.10371464)\n",
    "\n",
    "License: Creative Commons Attribution 4.0 International (CC BY 4.0)\n",
    "\n",
    "## Features:\n",
    "- Downloads and extracts the Fatura2 dataset\n",
    "- Converts images and annotations into a structured format\n",
    "- Creates train/dev/test splits using two different strategies\n",
    "- Saves the processed dataset locally in parquet format\n",
    "- Uploads the processed dataset to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8be091",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2223b-c0fe-4e40-a958-1f3c5d3988e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet huggingface_hub[hf_transfer]==0.27.1 datasets==3.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abeed2",
   "metadata": {},
   "source": [
    "## 2. Download and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375fb03bd0ee7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7108b2-be2f-4f9b-a033-8b54070304ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2d0fc-9795-4e34-852f-da5b298f4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Fatura2 dataset if not present\n",
    "![ -f {data_dir}/FATURA2.zip ] || curl https://zenodo.org/records/10371464/files/FATURA2.zip?download=1 -o {data_dir}/FATURA2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeaceeb-7a04-4f40-943a-3107e9911a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract dataset\n",
    "!unzip -n {data_dir}/FATURA2.zip -d {data_dir} || echo \"Failed to unzip or files already exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d024e",
   "metadata": {},
   "source": [
    "## 3. Define Data Loading Utilities\n",
    "\n",
    "Create helper functions to load and process the dataset files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78f6922c8cd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import mimetypes\n",
    "\n",
    "def load_files_from_csv(\n",
    "    csv_path: str,\n",
    "    base_dir_images: str = None,\n",
    "    base_dir_json: str = None,\n",
    "    template_inds=None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load and process files from CSV containing image and JSON paths.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the CSV file containing file references\n",
    "        base_dir_images: Base directory for image files\n",
    "        base_dir_json: Base directory for JSON annotation files\n",
    "        template_inds: Optional list of template indices to filter\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing processed files with images and annotations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Filter templates if specified\n",
    "        if template_inds:\n",
    "            df = df[\n",
    "                df[\"img_path\"].apply(\n",
    "                    lambda x: int(x.split(\"_\")[0].split(\"Template\")[1]) in template_inds\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # Set base paths\n",
    "        base_path_images = Path(base_dir_images) if base_dir_images else Path(csv_path).parent\n",
    "        base_path_json = Path(base_dir_json) if base_dir_json else Path(csv_path).parent\n",
    "\n",
    "        # Create full paths\n",
    "        df[\"full_img_path\"] = df[\"img_path\"].apply(lambda x: str(base_path_images / x))\n",
    "        df[\"full_annot_path\"] = df[\"annot_path\"].apply(lambda x: str(base_path_json / x))\n",
    "\n",
    "        def process_row(row: pd.Series) -> Dict[str, Any]:\n",
    "            try:\n",
    "                # Read image bytes\n",
    "                with open(row[\"full_img_path\"], \"rb\") as img_file:\n",
    "                    img_bytes = img_file.read()\n",
    "\n",
    "                # Read JSON annotation\n",
    "                with open(row[\"full_annot_path\"], \"r\") as json_file:\n",
    "                    json_dict = json.load(json_file)\n",
    "\n",
    "                json_dict.pop(\"OTHER\", None)\n",
    "                json_string = json.dumps(json_dict)\n",
    "                document_file = Path(row[\"full_img_path\"])\n",
    "                mime_type = mimetypes.guess_type(document_file)[0]\n",
    "\n",
    "                return {\n",
    "                    \"filename\": document_file.name,\n",
    "                    \"filetype\": mime_type,\n",
    "                    \"target_data\": json_string,\n",
    "                    \"doc_bytes\": img_bytes,\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing files for row {row.name}: {e}\")\n",
    "                return None\n",
    "\n",
    "        # Process all rows\n",
    "        results = df.apply(process_row, axis=1).tolist()\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0430e1f",
   "metadata": {},
   "source": [
    "## 4. Create Dataset Splits\n",
    "\n",
    "Process the data into train/dev/test splits using two different strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ddbb219df43857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Define splits\n",
    "splits = {\n",
    "    \"dev\": \"strat1_dev.csv\",\n",
    "    \"test\": \"strat1_test.csv\",\n",
    "    \"train\": \"strat1_train.csv\",\n",
    "}\n",
    "\n",
    "dataset_dir = f\"{data_dir}/invoices_dataset_final\"\n",
    "images_dir = f\"{dataset_dir}/colored_images\"\n",
    "annotations_dir = f\"{dataset_dir}/Annotations/Original_Format\"\n",
    "\n",
    "# Create Strategy 1 dataset (random split)\n",
    "dataset_strat1 = DatasetDict()\n",
    "for split_name, split_file in splits.items():\n",
    "    df = load_files_from_csv(\n",
    "        f\"{dataset_dir}/{splits['dev']}\", \n",
    "        images_dir, \n",
    "        annotations_dir, \n",
    "        None\n",
    "    )\n",
    "    dataset = Dataset.from_dict(df.to_dict(orient=\"list\"))\n",
    "    dataset_strat1[split_name] = dataset\n",
    "\n",
    "# Template indices for Strategy 2\n",
    "train_inds = set([\n",
    "    3, 11, 30, 24, 40, 48, 41, 22, 27, 19, 45, 1, 29, 44, 9, 47, 36, 23, 18, 42,\n",
    "    15, 14, 28, 43, 33, 6, 38, 26, 13, 34, 17, 37, 5, 8, 21, 35, 16, 20, 31, 46\n",
    "])\n",
    "dev_inds = set([50, 7, 32, 39, 2, 12, 4, 49, 10, 25])\n",
    "\n",
    "# Create Strategy 2 dataset (template-based split)\n",
    "dataset_strat2 = DatasetDict()\n",
    "for split_name, split_file in splits.items():\n",
    "    df = load_files_from_csv(\n",
    "        f\"{dataset_dir}/{splits['dev']}\",\n",
    "        images_dir,\n",
    "        annotations_dir,\n",
    "        train_inds if \"train\" in split_name else dev_inds,\n",
    "    )\n",
    "    dataset = Dataset.from_dict(df.to_dict(orient=\"list\"))\n",
    "    dataset_strat2[split_name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39d54d-8937-41df-9af1-aaf6de7813b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets inspect dataset with strategy 1\n",
    "dataset_strat1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc040f03",
   "metadata": {},
   "source": [
    "## 5. Inspect Sample Data\n",
    "\n",
    "View an example document and its JSON annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed87da6b808f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from IPython.display import JSON, Image\n",
    "import json\n",
    "\n",
    "sample_row = dataset_strat1[\"dev\"][3]\n",
    "target_data = sample_row[\"target_data\"]\n",
    "image = Image(data=sample_row[\"doc_bytes\"], width=400)\n",
    "\n",
    "print(\"\\nDocument Sample:\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414add76-d604-42af-bc24-bcd5cf354948",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JSON Annotation:\")\n",
    "JSON(json.loads(target_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6d3eb",
   "metadata": {},
   "source": [
    "## 6. Save and Upload to Hugging Face Hub\n",
    "\n",
    "Save the processed datasets locally and upload to Hugging Face Hub.\n",
    "You can get your HF token from - [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e96f0-b348-4628-8d98-87e71b8c6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable faster transfers\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad1556-4873-406e-b2f8-fae72720fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "from huggingface_hub import notebook_login, HfApi\n",
    "from huggingface_hub.errors import LocalTokenNotFoundError\n",
    "try:\n",
    "    print(f\"HF Name: {HfApi().whoami()['name']}\")\n",
    "except LocalTokenNotFoundError:\n",
    "    notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac33708c391f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and upload datasets\n",
    "dataset_name1 = \"Fatura2-invoices-original-strat1\"\n",
    "dataset_name2 = \"Fatura2-invoices-original-strat2\"\n",
    "#define hf username\n",
    "hf_username = \"arlind0xbb\" # adjust with your own hf_username\n",
    "\n",
    "# Save locally\n",
    "dataset_strat1.save_to_disk(f\"{data_dir}/{dataset_name1}.hf\")\n",
    "dataset_strat2.save_to_disk(f\"{data_dir}/{dataset_name2}.hf\")\n",
    "\n",
    "# Upload to Hugging Face Hub\n",
    "dataset_strat1.push_to_hub(f\"{hf_username}/{dataset_name1}\", private=True)\n",
    "dataset_strat2.push_to_hub(f\"{hf_username}/{dataset_name2}\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e4bde-fd24-46e0-9ecb-d6804d3a61b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
